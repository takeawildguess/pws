<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scikit-learn on take a wild guess</title>
    <link>https://takeawildguess.net/tags/scikit-learn/</link>
    <description>Recent content in scikit-learn on take a wild guess</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://takeawildguess.net/tags/scikit-learn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 8</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel8/</link>
      <pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel8/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have introduced the bias and variance concepts in Part1 and the bias-variance dilemma, the model capacity, the training/testing split practice and learning curves analysis in Part2.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 7</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel7/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel7/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have introduced the bias and variance concepts in Part1 and the bias-variance dilemma, the model capacity, the training/testing split practice and learning curves analysis in Part2.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 6</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel6/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel6/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have introduced the bias and variance concepts in Part1 and the bias-variance dilemma, the model capacity, the training/testing split practice and learning curves analysis in Part2.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 5</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel5/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel5/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have introduced the bias and variance concepts in Part1 and the bias-variance dilemma, the model capacity, the training/testing split practice and learning curves analysis in Part2.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 4</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel4/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel4/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have introduced the bias and variance concepts in Part1 and the bias-variance dilemma, the model capacity, the training/testing split practice and learning curves analysis in Part2.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 3</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel3/</link>
      <pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel3/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have introduced the bias and variance concepts in Part1 and the bias-variance dilemma, the model capacity, the training/testing split practice and learning curves analysis in Part2.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 2</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel2/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel2/</guid>
      <description>1. Introduction After two introductory post-series (linear and logistic regression), we dive into a crucial topic that every machine-learning practitioner should be at least aware of: model selection.
Basically, we do not want our models to learn our data by heart and then to struggle to handle new unseen data samples. We want them to be great at generalizing.
We have already introduced the two key terms that we need to deal with: bias and variance.</description>
    </item>
    
    <item>
      <title>Do not overlearn your data too much, learn to generalize - Part 1</title>
      <link>https://takeawildguess.net/blog/mdlsel/mdlsel1/</link>
      <pubDate>Sun, 14 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/mdlsel/mdlsel1/</guid>
      <description>1. Introduction In the two previous post-series about (linear and (logistic regression, we have used the whole dataset to train the model and to assess the final performance.
We now have the chance to go a bit deeper to get some understanding about model selection, why it is so crucial in the machine-learning field to prevent that expected-to-work-amazingly models or applications could miserably and/or dangerously fail.
A failure in a machine-learning application is not always the end of the world, especially if we want to classify whether a cat is in an uploaded picture or not.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 8</title>
      <link>https://takeawildguess.net/blog/logreg/logreg8/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg8/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 7</title>
      <link>https://takeawildguess.net/blog/logreg/logreg7/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg7/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 6</title>
      <link>https://takeawildguess.net/blog/logreg/logreg6/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg6/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 5</title>
      <link>https://takeawildguess.net/blog/logreg/logreg5/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg5/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We have analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 4</title>
      <link>https://takeawildguess.net/blog/logreg/logreg4/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg4/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We have analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 3</title>
      <link>https://takeawildguess.net/blog/logreg/logreg3/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg3/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
In this post, we implement the simple logistic regression case that we have analyzed in the first two posts (Part 1 and Part 2) using Sklearn first and Tensorflow then.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 5</title>
      <link>https://takeawildguess.net/blog/linreg/linreg5/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg5/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way here and extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome here.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 4</title>
      <link>https://takeawildguess.net/blog/linreg/linreg4/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg4/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behaviour identification from a bunch of data that are generated in a synthetic way here and extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome here.</description>
    </item>
    
  </channel>
</rss>