<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>matplotlib on take a wild guess</title>
    <link>https://takeawildguess.net/tags/matplotlib/</link>
    <description>Recent content in matplotlib on take a wild guess</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://takeawildguess.net/tags/matplotlib/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How does a neural network internally shape the space?</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn18/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn18/</guid>
      <description>1. Introduction Welcome back to the FCNN series!
In this new post, we are going to apply the same workflow used in the previous four posts (from this to that post) to visualise a 2D batch of inputs. We have defined and used this workflow to visualize a single-sample case. We refer to the single-sample case as 0D and to the batch case as 2D.
The batch contains the whole input space, which will be transformed throughout each neuron till the final output, i.</description>
    </item>
    
    <item>
      <title>Can we see inside a neural network?</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn14/</link>
      <pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn14/</guid>
      <description>1. Introduction Welcome back to the FCNN series!
In this new post, we are going to dig in and see what happens inside a feed-forward neural network that has been trained to return the right output.
We train the neural network on some basic examples with Tensorflow. If you are new to this library, please check these two posts out, 1 and 2, as well as my introductory post on linear regression and that one on neural networks.</description>
    </item>
    
  </channel>
</rss>