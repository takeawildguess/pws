<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Numpy on take a wild guess</title>
    <link>https://takeawildguess.net/tags/numpy/</link>
    <description>Recent content in Numpy on take a wild guess</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://takeawildguess.net/tags/numpy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 2</title>
      <link>https://takeawildguess.net/blog/logreg/logreg2/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg2/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
In this post, we implement the logistic regression theory that we have analyzed in the first post using Python and Numpy from scratch.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 3</title>
      <link>https://takeawildguess.net/blog/linreg/linreg3/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg3/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behaviour identification from a bunch of data that are generated in a synthetic way here.
We now extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 2</title>
      <link>https://takeawildguess.net/blog/linreg/linreg2/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg2/</guid>
      <description>1. Introduction We have introduced in the previous post the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion. In this post we apply the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way.
We are going to implement the logic from scratch in Python and its powerful numerical library, Numpy.</description>
    </item>
    
  </channel>
</rss>