<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on take a wild guess</title>
    <link>https://takeawildguess.net/tags/python/</link>
    <description>Recent content in python on take a wild guess</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://takeawildguess.net/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Can we visualize the flow of a regression neural network?</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn16/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn16/</guid>
      <description>1. Introduction Welcome back to the FCNN series!
In this new post, we are going to use the Python visualization class, visFCNN(), developed in the two previous posts. We want to see what happens inside a feed-forward neural network, which has been trained on toy examples with Tensorflow with the previously-developed Python class, trainFCNN(), for a regression problem.
The Python class, visFCNN(), takes as input a dictionary containing all the information required to visualize the network flow, namely the values of the network parameters and main nodes (inputs, linear outputs and activation outputs).</description>
    </item>
    
    <item>
      <title>How to build a Python class to visualize a neural network?</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn15/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn15/</guid>
      <description>1. Introduction Welcome back to the FCNN series!
In this new post, we are going to develop a Python class to visualize what happens inside a feed-forward neural network, which has been trained on toy examples with Tensorflow with the previously-developed Python class, trainFCNN(). The Python class, visFCNN(), takes as input a dictionary containing all the information required to visualize the network flow, namely the values of the network parameters and main nodes (inputs, linear outputs and activation outputs).</description>
    </item>
    
    <item>
      <title>Can we see inside a neural network?</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn14/</link>
      <pubDate>Sun, 24 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn14/</guid>
      <description>1. Introduction Welcome back to the FCNN series!
In this new post, we are going to dig in and see what happens inside a feed-forward neural network that has been trained to return the right output.
We train the neural network on some basic examples with Tensorflow. If you are new to this library, please check these two posts out, 1 and 2, as well as my introductory post on linear regression and that one on neural networks.</description>
    </item>
    
    <item>
      <title>Multi-hyperparameter analysis of a neural network and computational comparison</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn13/</link>
      <pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn13/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post we are going to:</description>
    </item>
    
    <item>
      <title>Hyperparameter analysis for regression</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn12/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn12/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze the hyperparameter (HP) space for a regression problem in Keras.</description>
    </item>
    
    <item>
      <title>Hyperparameter analysis for multi-class classification</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn11/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn11/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze the hyperparameter (HP) space for a multi-class classification problem in Keras.</description>
    </item>
    
    <item>
      <title>Meta-learning neural networks over basic tasks</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn10/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn10/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze the hyper-parameter space, which is referred to as meta-learning.</description>
    </item>
    
    <item>
      <title>How neural networks learn basic features with Pytorch</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn09/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn09/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze how to train a neural network on toy examples with Pytorch.</description>
    </item>
    
    <item>
      <title>Key notions of Pytorch</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn08/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn08/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to introduce the key components and modules of Pytorch, while we are going to use and apply to a neural network in the next one.</description>
    </item>
    
    <item>
      <title>How neural networks learn basic features with Tensorflow</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn07/</link>
      <pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn07/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze how to train a neural network on toy examples with Tensorflow.</description>
    </item>
    
    <item>
      <title>How neural networks learn basic features with Keras</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn06/</link>
      <pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn06/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze how to train a neural network on toy examples with Keras.</description>
    </item>
    
    <item>
      <title>How neural networks learn basic features with Scikit-learn</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn05/</link>
      <pubDate>Sun, 22 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn05/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this new post, we are going to analyze how to train a neural network on toy examples with Scikit-learn.</description>
    </item>
    
    <item>
      <title>How neural networks learn basic features - Create datasets</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn04/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn04/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The general series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In the following posts, we are going to analyze toy examples with advanced deep-learning libraries, namely Scikit-learn, Keras, Tensorflow and Pytorch.</description>
    </item>
    
    <item>
      <title>FCNN - Geometric intuition behind neural networks</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn03/</link>
      <pubDate>Sun, 08 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn03/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In the previous post, we gave some geometric insight into what occurs in a single neuron.</description>
    </item>
    
    <item>
      <title>FCNN - Geometric intuition behind a neuron</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn02/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn02/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
The series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!  In this post, we give some geometric insight into what occurs in a single neuron.</description>
    </item>
    
    <item>
      <title>Fully connected neural networks - cheat sheet</title>
      <link>https://takeawildguess.net/blog/fcnn/fcnn01/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/fcnn/fcnn01/</guid>
      <description>1. Introduction This post belongs to a new series of posts related to a huge and popular topic in machine learning: fully connected neural networks.
Plenty of books, lectures, tutorials and posts are available out there. The scope here is to analyze the topic with a slightly different perceptive.
The series scope is three-fold:
 visualize the model features and characteristics with schematic pictures and charts learn to implement the model with different levels of abstraction, given by the framework used have some fun with one of the hottest topics right now!</description>
    </item>
    
    <item>
      <title>Code art in Python - Spirograph pattern in a rectangle - Part 2</title>
      <link>https://takeawildguess.net/blog/codeart/codeart6/</link>
      <pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/codeart/codeart6/</guid>
      <description>1. Introduction This post belongs to the code art series, where art is built with code. The previous post and this one will aim at defining a function to create a geometric pattern, a spirograph pattern in a rectangle, inspired by this video, with Python libraries.
We will follow these steps:
 Grid definition, parametric equation of the rectangle in Part1. Polygon vertex detection, postprocessing of intersection points, colour scheme definition, global function implementation and final drawings in Part2.</description>
    </item>
    
    <item>
      <title>Code art in Python - Spirograph pattern in a rectangle - Part 1</title>
      <link>https://takeawildguess.net/blog/codeart/codeart5/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/codeart/codeart5/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of drawing both static and dynamic pictures with a programming language. This practice is commonly referred to as code art, where art is built with code.
This post and the next one will aim at defining a function to create a geometric pattern, a spirograph pattern in a rectangle, inspired by this video, with Python libraries.</description>
    </item>
    
    <item>
      <title>Code art in Python - Spirograph pattern in circle - Part 4</title>
      <link>https://takeawildguess.net/blog/codeart/codeart4/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/codeart/codeart4/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of drawing both static and dynamic pictures with a programming language. This practise is commonly referred to as code art, where art is built with code.
The first four posts will aim at defining a function to create a geometric pattern, a spirograph pattern in circle, inspired by this video, with Python libraries.</description>
    </item>
    
    <item>
      <title>Code art in Python - Spirograph pattern in circle - Part 3</title>
      <link>https://takeawildguess.net/blog/codeart/codeart3/</link>
      <pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/codeart/codeart3/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of drawing both static and dynamic pictures with a programming language. This practise is commonly referred to as code art, where art is built with code.
The first four posts will aim at defining a function to create a geometric pattern, a spirograph pattern in circle, inspired by this video, with Python libraries.</description>
    </item>
    
    <item>
      <title>Code art in Python - Spirograph pattern in circle - Part 2</title>
      <link>https://takeawildguess.net/blog/codeart/codeart2/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/codeart/codeart2/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of drawing both static and dynamic pictures with a programming language. This practise is commonly referred to as code art, where art is built with code.
The first four posts will aim at defining a function to create a geometric pattern, a spirograph pattern in circle, inspired by this video, with Python libraries.</description>
    </item>
    
    <item>
      <title>Code art in Python - Spirograph pattern in circle - Part 1</title>
      <link>https://takeawildguess.net/blog/codeart/codeart1/</link>
      <pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/codeart/codeart1/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of drawing both static and dynamic pictures with a programming language. This practise is commonly referred to as code art, where art is built with code.
One of the most popular languages I have seen so far to create some code art is Javascript, but I wish to investigate how Python libraries to reach the same goal.</description>
    </item>
    
    <item>
      <title>Learning Python by doing - Part 5</title>
      <link>https://takeawildguess.net/blog/kata/kata5/</link>
      <pubDate>Sun, 07 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/kata/kata5/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of learning a new programming language, such as Python, by doing. In the first two posts (Part1 and Part2), we develop a Python code from scratch to determine the travelled distance of a bike rider in a given time span, for a fixed and increasing annual distance target. We started to play with some text processing tasks in Part3.</description>
    </item>
    
    <item>
      <title>Learning Python by doing - Part 4</title>
      <link>https://takeawildguess.net/blog/kata/kata4/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/kata/kata4/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of learning a new programming language, such as Python, by doing. There are tons of material about Python tutorials for every level of expertise. It has been quite a while now that platforms such as codewars, topcoders and similar have been around. The nice thing that I like about these platforms is that the trainee is faced with many tasks or challenges of increasing difficulty.</description>
    </item>
    
    <item>
      <title>Learning Python by doing - Part 3</title>
      <link>https://takeawildguess.net/blog/kata/kata3/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/kata/kata3/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of learning a new programming language, such as Python, by doing. There are tons of material about Python tutorials for every level of expertise. It has been quite a while now that platforms such as codewars, topcoders and similar have been around. The nice thing that I like about these platforms is that the trainee is faced with many tasks or challenges of increasing difficulty.</description>
    </item>
    
    <item>
      <title>Learning Python by doing - Part 2</title>
      <link>https://takeawildguess.net/blog/kata/kata2/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/kata/kata2/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of learning a new programming language, such as Python, by doing. There are tons of material about Python tutorials for every level of expertise. It has been quite a while now that platforms such as codewars, topcoders and similar have been around. The nice thing that I like about these platforms is that the trainee is faced with many tasks or challenges of increasing difficulty.</description>
    </item>
    
    <item>
      <title>Learning Python by doing - Part 1</title>
      <link>https://takeawildguess.net/blog/kata/kata1/</link>
      <pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/kata/kata1/</guid>
      <description>1. Introduction This post belongs to a new series of posts where I intend to face the challenge of learning a new programming language, such as Python, by doing. There are tons of material about Python tutorials for every level of expertise. It has been quite a while now that platforms such as codewars, topcoders and similar have been around. The nice thing that I like about these platforms is that the trainee is faced with many tasks or challenges of increasing difficulty.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 8</title>
      <link>https://takeawildguess.net/blog/logreg/logreg8/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg8/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 7</title>
      <link>https://takeawildguess.net/blog/logreg/logreg7/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg7/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 6</title>
      <link>https://takeawildguess.net/blog/logreg/logreg6/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg6/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 5</title>
      <link>https://takeawildguess.net/blog/logreg/logreg5/</link>
      <pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg5/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We have analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 4</title>
      <link>https://takeawildguess.net/blog/logreg/logreg4/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg4/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We have analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 3</title>
      <link>https://takeawildguess.net/blog/logreg/logreg3/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg3/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
In this post, we implement the simple logistic regression case that we have analyzed in the first two posts (Part 1 and Part 2) using Sklearn first and Tensorflow then.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 2</title>
      <link>https://takeawildguess.net/blog/logreg/logreg2/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg2/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
In this post, we implement the logistic regression theory that we have analyzed in the first post using Python and Numpy from scratch.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 5</title>
      <link>https://takeawildguess.net/blog/linreg/linreg5/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg5/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way here and extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome here.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 4</title>
      <link>https://takeawildguess.net/blog/linreg/linreg4/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg4/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behaviour identification from a bunch of data that are generated in a synthetic way here and extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome here.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 3</title>
      <link>https://takeawildguess.net/blog/linreg/linreg3/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg3/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behaviour identification from a bunch of data that are generated in a synthetic way here.
We now extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 2</title>
      <link>https://takeawildguess.net/blog/linreg/linreg2/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg2/</guid>
      <description>1. Introduction We have introduced in the previous post the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion. In this post we apply the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way.
We are going to implement the logic from scratch in Python and its powerful numerical library, Numpy.</description>
    </item>
    
  </channel>
</rss>