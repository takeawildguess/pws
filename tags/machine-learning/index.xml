<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on take a wild guess</title>
    <link>https://takeawildguess.net/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on take a wild guess</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 24 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://takeawildguess.net/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 6</title>
      <link>https://takeawildguess.net/blog/logreg/logreg6/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg6/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems with logistic regression algorithms. Classification entails that the output is a discrete variable taking values on a predefined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
We analyzed the theory in the first post, implement the algorithm with Numpy in Part 2 and using Sklearn and Tensorflow in Part 3.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 3</title>
      <link>https://takeawildguess.net/blog/logreg/logreg3/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg3/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
In this post, we implement the simple logistic regression case that we have analyzed in the first two posts (Part 1 and Part 2) using Sklearn first and Tensorflow then.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 2</title>
      <link>https://takeawildguess.net/blog/logreg/logreg2/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg2/</guid>
      <description>1. Introduction and assumptions In this post-series, we are going to study the very basic modelling for classification problems, the logistic regression. Classification entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are spam detection, object recognition and topic identification.
In this post, we implement the logistic regression theory that we have analyzed in the first post using Python and Numpy from scratch.</description>
    </item>
    
    <item>
      <title>Learning to classify coffee from cappuccino - Part 1</title>
      <link>https://takeawildguess.net/blog/logreg/logreg1/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/logreg/logreg1/</guid>
      <description>replace media/ to /blog/logreg/logreg1/
1. Problem formulation We want to create a simplified representation (model) of real/world from available data. The model architecture can be seen as a box that takes some quantities as input, performs some internal computation and returns some other quantities as output. The inputs are also referred to as features or predictors of the problem to solve, since they contain valuable information that the model should exploit to come up with the correct outcome.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 3</title>
      <link>https://takeawildguess.net/blog/linreg/linreg3/</link>
      <pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg3/</guid>
      <description>1. Introduction We have introduced the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion in this post, while we have applied the theory to a simple but practical case of linear-behaviour identification from a bunch of data that are generated in a synthetic way here.
We now extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 2</title>
      <link>https://takeawildguess.net/blog/linreg/linreg2/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg2/</guid>
      <description>1. Introduction We have introduced in the previous post the concept of the linear-regression problem and the structure to solve it in a &amp;ldquo;machine-learning&amp;rdquo; fashion. In this post we apply the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way.
We are going to implement the logic from scratch in Python and its powerful numerical library, Numpy.</description>
    </item>
    
    <item>
      <title>Hello world for Machine learning - Part 1</title>
      <link>https://takeawildguess.net/blog/linreg/linreg1/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://takeawildguess.net/blog/linreg/linreg1/</guid>
      <description>1. Problem formulation We want to create a simplified representation (model) of real/world from available data. The model architecture can be seen as a box that takes some quantities as input, performs some internal computation and returns some other quantities as output. The inputs are also referred to as features or predictors of the problem to solve, since they contain valuable information that the model should exploit to come up with the correct outcome.</description>
    </item>
    
  </channel>
</rss>