<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

   <meta name="description" content="Personal website"> 
   <meta name="author" content="Your name"> 
  

  <meta name="generator" content="Hugo 0.49" />
  <title>How to learn to classify - Part 2 &middot; take a wild guess</title>

  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">


 <link rel="stylesheet" href="https://takeawildguess.net/css/main.css"> 


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.css">

  
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>



    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script> 
    <script>hljs.initHighlightingOnLoad();</script>






<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>



<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.js"></script>


<script type="text/javascript">
  window.onscroll = function() {myFunction()};
  function myFunction() {
    var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
    var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    var scrolled = (winScroll / height) * 100;
    document.getElementById("myBar").style.width = scrolled + "%";
  }
</script>


<script type="text/javascript">
  $(document).ready(function(){
    $(window).scroll(function () {
      if ($(this).scrollTop() > 50) { $('#back-to-top').fadeIn(); } else { $('#back-to-top').fadeOut(); }
    });
    
    $('#back-to-top').click(function () {
      $('#back-to-top').tooltip('hide');
      $('body,html').animate({ scrollTop: 0 }, 800); return false; });
    $('#back-to-top').tooltip('show');
  })
</script>


  
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">
  

  
    <link rel="shortcut icon" type="image/x-icon" href="https://takeawildguess.net/images/logo/twgLogo.png">
  

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  

  
  
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  

</head>

  <!-- Navigation -->
<nav class="navbar fixed-top navbar-expand-md navbar-dark bg-dark">
  
    <a class="navbar-brand abs" href="https://takeawildguess.net/">
      <img src="https://takeawildguess.net/images/logo/twgLogo.png" class="img-responsive" id="nav-logo" alt="How to learn to classify - Part 2">
    </a>
  
  <a class="navbar-brand" href="/blog/logisticregression_part2/" style="font-size: 16px; ">Is it coffee or cappuccino?</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsingNavbar">
      <span class="navbar-toggler-icon"></span>
  </button>
  <div class="navbar-collapse collapse" id="collapsingNavbar">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/">Home <span class="sr-only">(current)</span></a></li>
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/blog/">Blog</a></li>
        
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="https://takeawildguess.net/about/" id="navbarDropdown" role="button"
          data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            <a class="dropdown-item" href="https://takeawildguess.net/about/">Main</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_twg">TWG</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_me">Me</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_skl">Skills</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_exp">Experience</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_pt">Talks</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/resume/">Resume</a>
          </div>
        </li>

        
      </ul>
      
      <ul class="navbar-nav navbar-right">
        
          <li class="nav-item navbar-icon"><a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a></li>
        
      </ul>
      
  </div>
  <div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
  </div>
</nav>

  <body data-spy="scroll" data-target="#toc">
    <!-- Hero -->
<header class="postHead">
  <div class="container-fluid overlay">
    <div class="descr">
      <img src="https://takeawildguess.net/blog/images/coffee_cappuccino.jpeg" class="img-fluid" alt="__">
      <div class="card">
        <div class="card-body">
          <h1 class="card-title text-center">How to learn to classify - Part 2</h1>
          <h6 class="card-text text-center">February 10, 2019</h6>
          <h6 class="card-text text-center"><span class="fa fa-clock-o"></span> 17 min read</h6>
          <h6 class="card-text text-center">
            <a href="https://takeawildguess.net/tags/logistic-regression"><kbd class="item-tag">logistic-regression</kbd></a> <a href="https://takeawildguess.net/tags/algorithm"><kbd class="item-tag">algorithm</kbd></a> <a href="https://takeawildguess.net/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a> <a href="https://takeawildguess.net/tags/python"><kbd class="item-tag">python</kbd></a> <a href="https://takeawildguess.net/tags/scikit-learn"><kbd class="item-tag">scikit-learn</kbd></a> <a href="https://takeawildguess.net/tags/tensorflow"><kbd class="item-tag">tensorflow</kbd></a> 
          </h6>
        </div>
      </div>
    </div>
  </div>
</header>

    <section class="postContent">
  <div class="container">
    <div class="row">
      
      <div class="col-sm-2 col-lg-2">
        <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
      </div>
      
      <div class="col-lg-10 col-sm-10">
        <div class="container blogPost">
          

<h2 id="1-introduction-and-assumptions">1. Introduction and assumptions</h2>

<p>In this post-series, we are going to study the very basic modeling for classification problems, the logistic regression.
<strong>Classification</strong> entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are <em>spam detection</em>, <em>object recognition</em> and <em>topic identification</em>.</p>

<p>We focus on the input/output space, the predictor structure, the learning algorithm and on applying the method to different datasets.
In this series, we do not split the dataset into training and testing sets, but we assess every model on the training set only.
A dedicated post on model selection, overfitting/underfitting problem and regularization will be published soon.</p>

<p>Here follows the post-series steps:</p>

<ol>
<li>Probabilistic model of a classification problem and cross-entropy definition via maximum likelihood (<a href="/blog/logisticregression_part1/">Part 1</a>).</li>
<li>Logistic regression developed from scratch with Python and Numpy (<a href="/blog/logisticregression_part1/">Part 1</a>).</li>
<li>Logistic regression implementation in Scikit-learn and TensorFlow (Part 2).</li>
<li>How to model different predictor spaces, namely 1D, 2D and 3D (Part 2).</li>
<li>Multinomial classification, where both softmax function and one-vs-all method are applied and compared (<a href="/blog/logisticregression_part3/">Part 3</a>).</li>
<li>Non-linear input predictors (<a href="/blog/logisticregression_part3/">Part 3</a>).</li>
<li>Categorical and numerical predictors (<a href="/blog/logisticregression_part4/">Part 4</a>).</li>
<li>Logistic regression applied to the <strong>digits</strong> dataset (<a href="/blog/logisticregression_part4/">Part 4</a>).</li>
<li>Logistic regression applied to the <strong>MNIST</strong> dataset (<a href="/blog/logisticregression_part4/">Part 4</a>).</li>
</ol>

<h2 id="2-scikit-learn-and-tensorflow-implementation">2. Scikit-learn and TensorFlow implementation</h2>

<p>We implement the simple logistic regression case that we have analyzed in the <a href="/blog/logisticregression_part1/">previous</a> post using Scikit/learn first and Tensorflow then.</p>

<h3 id="2-1-data-generation">2.1 Data generation</h3>

<p>We create an equispaced x grid with 1000 points ranging from -20 to 20.
Then true y outcome, which represents the class to assign to the input, is defined via the logistic function of the linear transformation of input x with parameters <em>Wgt</em>.
The actual class is assigned by drawing samples from a <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.binomial.html">binomial distribution</a>.
A binomial distribution becomes a Bernoulli distribution when the number of trials is 1.
In other words, we generate data by sampling the y class for each input x, where the probability of success (i.e., of belonging to class 1) is given by:</p>

<p>$ p = \sigma(\lbrack x, 1\rbrack\cdot \omega) $</p>

<p>The <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a> is the $\sigma$ operator in the equation.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from mpl_toolkits import mplot3d

def logistic(XX, ww, noise=0):
    tt = np.dot(XX, ww) + noise
    return 1/(1 + np.exp(-tt))

xx = np.linspace(-20., 20, 1000)
bb = np.ones_like(xx)
XX = np.vstack([xx, bb]).T # Add intercept
Wgt = np.array([2, -15]).reshape(-1, 1) # ground-truth parameters
Ymean = logistic(XX, Wgt) # True mean
Ynoise = logistic(XX, Wgt, noise=np.random.normal(scale=0.5, size=(len(xx), 1))) # noise
Yclass = np.random.binomial(1., Ynoise) # dichotomous variable, n_trial=1 =&gt; Bernoulli distribution
</code></pre>

<p>Figure shows the y class of each input in blue, the noise probability of success in green and the true mean with solid red line.</p>

<pre><code class="language-python">plt.figure(figsize=(12,6))
plt.plot(xx, Ymean, 'r-', lw=2, label='true mean curve')
plt.scatter(xx, Ynoise, c='g', label='noise curve')
plt.scatter(xx, Yclass, c='b', label='Y-class, dichotomous variable')
plt.legend();
</code></pre>

<p><img src="/blog/logReg2/output_7_0.png" alt="png" /></p>

<h3 id="2-2-sklearn-implementation">2.2 Sklearn implementation</h3>

<p>We implment the logistic regression <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">class</a> and the metric module directly from Sklearn.
Since we ignore regularization at this point, we set the <em>C</em> parameter to a very high value to reduce the regularization strength down to 0.
It is in fact the inverse of regularization strength, as it is the case for <a href="https://scikit-learn.org/stable/modules/svm.html">Support Vector Machine</a> applications.</p>

<p>After creating an instance of the model, we train it on the data with the fit method. The information learned from the data are automatically stored into the model instance, which is in this case <em>lgr</em>.
Sklearn returns the parameter configuration of the model instance.</p>

<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn import metrics
</code></pre>

<pre><code class="language-python">lgr = LogisticRegression(C=1e5) # we want to ignore regularization
YY = Yclass[:, 0]
lgr.fit(XX, YY)
</code></pre>

<pre><code>LogisticRegression(C=100000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
</code></pre>

<p>We then predict the labels of the same data input to assess the model performance at reproducing the training dataset.
It uses the information learned during the training process (fit step).
The predict method returns the class each input belongs according to the model, while predict_proba returns the probability to belong to any of the classes.
That&rsquo;s why you get always 1 if you sum that 2D array along the horizontal axis.</p>

<pre><code class="language-python">Ypred = lgr.predict(XX)
Ypred_prob = lgr.predict_proba(XX)
Ypred.shape, Ypred_prob.shape
</code></pre>

<pre><code>((1000,), (1000, 2))
</code></pre>

<pre><code class="language-python">np.all(np.sum(Ypred_prob, axis=1)==1)
</code></pre>

<pre><code>True
</code></pre>

<p>We retrieve the model parameters and assess the model in terms of accuracy, precision and recall.</p>

<pre><code class="language-python">print(&quot;Parameters of single predictor: {:.3f}, intercept of decision boundary: {:.3f}&quot;.format(lgr.coef_[0,0], lgr.intercept_[0]))
</code></pre>

<pre><code>Parameters of single predictor: 1.837, intercept of decision boundary: -6.915
</code></pre>

<p>We define here the metrics to assess the model performance:</p>

<ol>
<li>accuracy, which is the fraction of correct predictions to the total number of data points,</li>
<li>precision, which is the fraction of correct predictions to the total number of positively predicted points,</li>
<li>recall, which is the fraction of correct predictions to the total number of positive points.</li>
</ol>

<p>For binary classification, four outcomes only are possible, where the positive (negative) class refers to value 1 (0):</p>

<ol>
<li>True positive (TP): the model correctly predicts class 1</li>
<li>False positive (FP): the model incorrectly predicts class 1</li>
<li>True negative (TN): the model correctly predicts class 0</li>
<li>False negative (FN): the model incorrectly predicts class 0</li>
</ol>

<p>Positive/negative refers to the model outcome, True/False to the correctness of the model prediction.</p>

<p>The definition of precision $P$, recall $R$ and accuracy $A$ are:</p>

<p>$ P = TP/(TP+FP) $</p>

<p>$ R = TP/(TP+FN) $</p>

<p>$ A = (TP+TN)/(TP+TN+FP+FN) $</p>

<p>See more details <a href="https://en.wikipedia.org/wiki/Precision_and_recall">here</a>.</p>

<pre><code class="language-python">print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred)))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred)))
</code></pre>

<pre><code>Accuracy: 0.978
Precision: 0.9647435897435898
Recall: 0.9647435897435898
</code></pre>

<p>Figure shows the ground-truth class of each input in blue, the model predicted class in red and the beneath probability of success with solid green line.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 6))
plt.scatter(xx, Yclass, c='b', s=60, alpha=0.6, label='Ground-truth')
plt.scatter(xx, Ypred, c='r', s=20, alpha=0.5, label='Dichotomous response')
plt.plot(xx, Ypred_prob[:,1], 'g', lw=2, label='Response probability')
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.legend()
plt.show()
</code></pre>

<p><img src="/blog/logReg2/output_19_0.png" alt="png" /></p>

<h3 id="2-3-tensorflow-implementation">2.3 TensorFlow implementation</h3>

<p>We import the entire library, from which we access to the various methods required to describe the model, to train it to the dataset and to estimate the outputs that are compared to the dataset ground-truth values.</p>

<p>The very first step is to reset the TF to the default graph, which means TF clears the default graph stack and resets the global default graph.</p>

<p>We then define the <em>x</em> and <em>y</em> variables as <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder"><strong>placeholder</strong></a>, while the <em>ww</em> parameters as <a href="https://www.tensorflow.org/guide/variables"><strong>variable</strong></a>.</p>

<p>In short, <em>tf.Variable</em> is used for trainable parameters of the model, while <em>tf.placeholder</em> is used to feed actual training examples.
That&rsquo;s why we need to assign initial values, often random-generated, to the TF variables only.
The variable values can therefore be updated during optimization, can be shared and be stored after training.
We assign the placeholder type as <em>float32</em> to both input and output.
The size of the input placeholder, <em>xp</em>, is set to (None, 2), since the number of rows is automatically determined from the batch size we feed to the optimizer object in the training step, while the column size is equal to the number of features (2 for the first case).
The size of the output placeholder is instead set to (None, 1), since only one value is required for each sample.</p>

<p>The feature weights <em>ww</em> and bias <em>bb</em>, which is equivalent to the Sk-Learn intercept, are defined with the <em>Variable</em> method and initialized as a (2,1) and a (1,1) zero-arrays, respectively.</p>

<p>The final step is to combine TF variables and placeholders to translate the mathematical model into code.
The matrix multiplication between the input matrix and the weight array is performed with <em>matmul</em>.
The probability of belonging to class 1 is given by the sigmoid function implemented in TF.
The class is obtained with the round operator, as:</p>

<p>$ Y_{class} = 1_{{p&gt;0.5}} $</p>

<p>where the $ 1_x $ operator returns 1 when x is True and 0 otherwise.</p>

<p>Accuracy is then defined by checking how frequently the model outcome and the ground-truth classes match each other.</p>

<p>At the end of these steps, we inspect the shape of each tensor.
The question-mark symbol says that TF needs some data to determine the actual row size.</p>

<pre><code class="language-python">import tensorflow as tf

tf.reset_default_graph()
xp = tf.placeholder(dtype=tf.float32, shape=(None, 2))
yp = tf.placeholder(dtype=tf.float32, shape=(None, 1))
ww = tf.Variable(np.zeros((2,1)), dtype=tf.float32)
ymdl = tf.matmul(xp, ww)
yprob = tf.sigmoid(ymdl)
ycls = tf.round(yprob)
accuracy = tf.reduce_mean(tf.cast(tf.equal(ycls, yp), dtype=tf.float32))
</code></pre>

<pre><code class="language-python">print('Input shape: {}'.format(xp.shape))
print('Ground-truth output shape: {}'.format(yp.shape))
print('Weight shape: {}'.format(ww.shape))
print('Model output shape: {}'.format(ymdl.shape))
</code></pre>

<pre><code>Input shape: (?, 2)
Ground-truth output shape: (?, 1)
Weight shape: (2, 1)
Model output shape: (?, 1)
</code></pre>

<p>The loss function is easily implemented using the method <em>sigmoid_cross_entropy_with_logits</em> from <em>losses</em> package.
The optimizer object that actually adjusts the model parameters (TF variables) with the gradient descent algorithm.</p>

<pre><code class="language-python">mdlLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=ymdl, labels=yp))
optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss=mdlLoss)
</code></pre>

<p>The next steps to <strong>train</strong> the model are to:</p>

<ol>
<li>initialize the variables.</li>
<li>run a new session, which let us perform the actual computation by exploiting the graph structure previously defined.</li>
<li>run the optimizer as many steps as the number of epochs <em>Nepoch</em>.</li>
<li>run the model with the final parameter set and store the model output <em>ycls</em> into the prediction array.</li>
<li>retrieve the final parameter values by running a dedicated session. A different way would be to call the <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables">global_variables()</a> method and get the variable values by key name.</li>
</ol>

<pre><code class="language-python">Nepoch = 10000
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    Jevol = []
    for kk in range(Nepoch):
        mdl_loss, _ = sess.run([mdlLoss, optimizer], feed_dict={xp: XX, yp: Yclass})
        if kk%100 == 0:
            Jevol.append((kk, mdl_loss))
        if kk==Nepoch-1:
            print('The final model loss is {}'.format(mdl_loss))

    Ypred_tf = sess.run(ycls, feed_dict={xp: XX})
    Yprob_tf = sess.run(yprob, feed_dict={xp: XX})
    wOpt = sess.run([ww])[0]
    mdlAcc = sess.run([accuracy], feed_dict={xp: XX, yp: Yclass})[0]
</code></pre>

<pre><code>The final model loss is 0.04472097009420395
</code></pre>

<p>We retrieve the model parameters and assess the model in terms of accuracy, precision and recall.</p>

<pre><code class="language-python">print(&quot;Parameter of single predictor: {:.3f}, intercept of decision boundary: {:.3f}&quot;.format(wOpt[0,0], wOpt[1,0]))
</code></pre>

<pre><code>Parameter of single predictor: 1.837, intercept of decision boundary: -13.830
</code></pre>

<pre><code class="language-python">print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred_tf)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred_tf)))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred_tf)))
</code></pre>

<pre><code>Accuracy: 0.978
Precision: 0.9647435897435898
Recall: 0.9647435897435898
</code></pre>

<p>Figure shows the ground-truth class of each input in blue, the model predicted class in red and the beneath probability of success with solid green line.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 6))
plt.scatter(xx, Yclass, c='b', s=60, alpha=0.6, label='Ground-truth')
plt.scatter(xx, Ypred_tf, c='r', s=20, alpha=0.5, label='Dichotomous response')
#plt.plot(xx, Ypred, 'r', lw=2, label='LSE')
plt.plot(xx, Yprob_tf, 'g', lw=2, label='Response probability')
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.legend()
plt.show()
</code></pre>

<p><img src="/blog/logReg2/output_32_0.png" alt="png" /></p>

<h2 id="3-input-space-2d-and-3d">3. Input space (2D and 3D)</h2>

<p>Here we show how the logistic regression model handles different dimensions of the predictor space.
However, the problem is still binary classification since the y response can only take 0 or 1 values.</p>

<h3 id="3-1-two-dimentional-space-of-predictors">3.1 Two-dimentional space of predictors</h3>

<p>We create the Bernoulli distribution for the two predictors, x1 and x2, that range from -10 to 10.</p>

<pre><code class="language-python">Npntx, Npnty = 50, 50 # number of points
x1_ = np.linspace(-10, 10, Npntx)
x2_ = np.linspace(-10, 10, Npnty)
xx1, xx2 = np.meshgrid(x1_, x2_)
noise = 0.25*(np.random.randn(Npnty,Npntx)-1)
w0, w1, w2 = 5, -3, -1  # ground-truth parameters
hh = w0 + w1*xx1 + w2*xx2 + noise
Ynoise = 1/(1+np.exp(-hh))
Ycls = np.random.binomial(1., Ynoise) # dichotomous variable, n_trial=1 =&gt; Bernoulli distribution
</code></pre>

<p>The dataset is created by stacking the flattened version of the two 2D predictor arrays and the corresponding class array.
We have therefore $50*50$ examples over the rows, two predictors over the X columns and the response over the Y column.</p>

<pre><code class="language-python">XX = np.vstack((xx1.flatten(), xx2.flatten())).T
Ycls = Ycls.flatten().reshape(-1,1)
print([XX.shape, Ycls.shape])
</code></pre>

<pre><code>[(2500, 2), (2500, 1)]
</code></pre>

<p>Figure reports the two class distribution with yellow/purple colors.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;);
</code></pre>

<p><img src="/blog/logReg2/output_39_0.png" alt="png" /></p>

<p>One of the amazing features of Scikit-learn concerns its general structure that defines the training/prediction process into four steps and requires little or no adjustment for slightly different model settings.</p>

<pre><code class="language-python">lgr = LogisticRegression(C=1e5) # we want to ignore regularization
YY = Ycls[:, 0]
lgr.fit(XX, YY)
Ypred = lgr.predict(XX)
Ypred_prob = lgr.predict_proba(XX)
Ypred.shape, Ypred_prob.shape
</code></pre>

<pre><code>((2500,), (2500, 2))
</code></pre>

<p>We retrieve the model parameters and realise that they are very close to the initial setting used to generate the dataset.</p>

<pre><code class="language-python">print(&quot;predictor parameters: ({:.3f},{:.3f}), intercept of decision boundary: {:.3f}&quot;\
      .format(lgr.coef_[0,0], lgr.coef_[0,1], lgr.intercept_[0]))
</code></pre>

<pre><code>predictor parameters: (-3.405,-1.152), intercept of decision boundary: 5.198
</code></pre>

<p>That can be also confirmed by assessing the model in terms of accuracy, precision and recall and comparing them to the data noise level.</p>

<pre><code class="language-python">print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred)))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred)))
</code></pre>

<pre><code>Accuracy: 0.9796
Precision: 0.9819193324061196
Recall: 0.9826026443980515
</code></pre>

<p>Figure reports the original dataset and the decision boundary as a group of green points.
It has been determined by taking the second column of <em>Ypred_prob</em>, reshaping to the shape of one of the two predictors, measuring the distance from the 50% threshold and selecting points that are not more far than 20%.
It means the decision boundary is 40% thick, from 30% to 70%.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis', label='data', alpha=0.4)
thickDecBound = np.abs(Ypred_prob[:,1].reshape(-1, xx1.shape[-1])-0.5)&lt;0.2 # 0.4-thick decision boundary
plt.scatter(xx1[thickDecBound], xx2[thickDecBound], c='g', s=40, alpha=0.8, label='0.4-thick decision boundary')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
plt.legend();
</code></pre>

<p><img src="/blog/logReg2/output_47_0.png" alt="png" /></p>

<h3 id="3-2-three-dimentional-space-of-predictors">3.2 Three-dimentional space of predictors</h3>

<p>We create the Bernoulli distribution for the three predictors, x1, x2 and x3, that range from -5 to 5.</p>

<p>The dataset is a random-generated 3D array, where rows are the number of samples.</p>

<pre><code class="language-python">Nx, Ny, Nz = 30, 20, 25
Npnt = 1000
xyz = 5*(2*np.random.rand(Npnt, 3)-1)
xx1, xx2, xx3 = xyz[:,0], xyz[:,1], xyz[:,2]
w0, w1, w2, w3 = 2, 3, -1, 3
noise = 0.05*(np.random.randn(Npnt)-1)
hh = w0 + w1*xx1 + w2*xx2 + w3*xx3 + noise
Ynoise = 1/(1+np.exp(-hh))
Ycls = np.random.binomial(1., Ynoise) # dichotomous variable, n_trial=1 =&gt; Bernoulli distribution
XX = xyz.copy() #np.vstack((xx1.flatten(), xx2.flatten(), xx3.flatten())).T
Ycls = Ycls.flatten().reshape(-1,1)
print([XX.shape, Ycls.shape])
</code></pre>

<pre><code>[(1000, 3), (1000, 1)]
</code></pre>

<p>Figure reports the two class distribution with yellow/purple colors in the 3D space.
This plotting is enabled with the module <a href="https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html"><em>mplot3d</em></a>.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.scatter(xx1, xx2, xx3, c=Ycls, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('x3')
ax.view_init(20, 25)
plt.show()
</code></pre>

<p><img src="/blog/logReg2/output_51_0.png" alt="png" /></p>

<p>We utilize the exact same structure also to build the 3D model.</p>

<pre><code class="language-python">lgr = LogisticRegression(C=1e5) # we want to ignore regularization
YY = Ycls[:, 0]
lgr.fit(XX, YY)
Ypred = lgr.predict(XX)
Ypred_prob = lgr.predict_proba(XX)
Ypred.shape, Ypred_prob.shape
</code></pre>

<pre><code>((1000,), (1000, 2))
</code></pre>

<p>We retrieve the model parameters and realise that they are very close to the initial setting used to generate the dataset.
That can be also confirmed by assessing the model in terms of accuracy, precision and recall and comparing them to the data noise level.</p>

<pre><code class="language-python">print(&quot;predictor parameters: ({:.3f},{:.3f},{:.3f}), intercept of decision boundary: {:.3f}&quot;\
      .format(lgr.coef_[0,0], lgr.coef_[0,1], lgr.coef_[0,2], lgr.intercept_[0]))
print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred)))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred)))
</code></pre>

<pre><code>predictor parameters: (2.983,-1.023,2.857), intercept of decision boundary: 2.045
Accuracy: 0.952
Precision: 0.9656419529837251
Recall: 0.9484902309058615
</code></pre>

<p>Figure reports the original dataset and the decision boundary as a plane whose mesh color is proportional to the x3 predictor.
The plane represents the set of predictor points for which the probability of success is equal to 50%.
That means the hypothesis function, which is fed to the logistic function, is equal to 0:</p>

<p>$ h = \omega_0 + \omega_1\cdot x_1 + \omega_2\cdot x_2 + \omega_3\cdot x_3 = 0 $</p>

<p>The plane can be described as an explit function between the x3 predictor and the other two inputs:</p>

<p>$ x_3 = -(\omega_0 + \omega_1\cdot x_1 + \omega_2\cdot x_2)/\omega_3 $</p>

<p>It is clear how the plane separates the two classes nicely.</p>

<pre><code class="language-python">Npntx, Npnty, mrg = 20, 20, 4 # number of points
x1_ = np.linspace(-mrg, mrg, Npntx)
x2_ = np.linspace(-mrg, mrg, Npnty)
mx1, mx2 = np.meshgrid(x1_, x2_)
zz = -(lgr.intercept_[0] + lgr.coef_[0,0]*mx1 + lgr.coef_[0,1]*mx2)/lgr.coef_[0,2]

plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(mx1, mx2, zz, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.4)
ax.scatter(xx1, xx2, xx3, c=Ycls, cmap='viridis', linewidth=0.5, alpha=0.6, label='dataset')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('x3')
ax.view_init(5, 65)
plt.legend()
plt.show()
</code></pre>

<p><img src="/blog/logReg2/output_57_0.png" alt="png" /></p>

<h2 id="reference">Reference</h2>

<ol>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">CS229 notes</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning at Coursera</a></li>
<li><a href="http://people.math.gatech.edu/~ecroot/3225/maximum_likelihood.pdf">Maximum likelihood estimators and least squares</a></li>
<li><a href="https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf">An Introduction to Statistical Learning</a></li>
<li><a href="https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression/46525">Generating artificial data</a></li>
<li><a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">Logistic Regression overview</a></li>
<li><a href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/">Logistic Regression for Machine learning</a></li>
<li><a href="https://data.princeton.edu/wws509/notes/c3.pdf">Princeton notes</a></li>
<li><a href="https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8">Building A Logistic Regression in Python, Step by Step</a></li>
<li><a href="https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a">Logistic Regression using Python (scikit-learn)</a></li>
</ol>

        </div>
        <div class="pgNav PageNavigation col-12 text-center">
          <span>
          <p>&laquo; <a class="" href="/blog/logisticregression_part1/" style="color: #4ABDAC; font-size: 18px; "> How to learn to classify - Part 1</a>

          
          &nbsp;&nbsp; | &nbsp;&nbsp;
          <a class="" href="/blog/logisticregression_part3/" style="color: #4ABDAC; font-size: 18px; ">How to learn to classify - Part 3</a>
          
          &raquo;</p>
          </span>
          
        </div>

      </div>
    </div>
  </div>
</section>



    
    

    

    <footer class="pgFoot">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 text-center icons">
        
        <span>
          <a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a>
        </span>
        
      </div>

      <hr style="border: 2px solid #4ABDAC; min-width: 250px; border-radius: 2px; " />
      <div class="col-12 text-center">
        <p>
          &bull; Copyright &copy; 2019, <a href="https://takeawildguess.net/">Mattia Venditti</a> &bull; All rights reserved. &bull;
          <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
          </a>
          All blog posts are released under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </p>
      </div>
      

      <div class="col-6 text-center">
        <p>Disclaimer: The views and opinions on this website are my own and do not reflect or represent the views of my employer.</p>
      </div>
      <div class="col-6 text-center">
        <p>
        Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://pages.github.com/">GitHub Pages</a>.
        The favicon and logo were created by myself.
        </p>
      </div>

    </div>
  </div>
</footer>

<a id="back-to-top" href="https://takeawildguess.net/" class="btn btn-primary btn-lg back-to-top" role="button" title="Click to return on the top page"
data-toggle="tooltip" data-placement="left"><i class="fa fa-angle-up"></i></a>


  </body>
</html>
