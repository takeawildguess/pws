<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

   <meta name="description" content="Personal website"> 
   <meta name="author" content="Your name"> 
  

  <meta name="generator" content="Hugo 0.49" />
  <title>How to learn to classify - Part 3 &middot; take a wild guess</title>

  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">


 <link rel="stylesheet" href="https://takeawildguess.net/css/main.css"> 


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.css">

  
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>



    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script> 
    <script>hljs.initHighlightingOnLoad();</script>






<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>



<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.js"></script>


<script type="text/javascript">
  window.onscroll = function() {myFunction()};
  function myFunction() {
    var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
    var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    var scrolled = (winScroll / height) * 100;
    document.getElementById("myBar").style.width = scrolled + "%";
  }
</script>


<script type="text/javascript">
  $(document).ready(function(){
    $(window).scroll(function () {
      if ($(this).scrollTop() > 50) { $('#back-to-top').fadeIn(); } else { $('#back-to-top').fadeOut(); }
    });
    
    $('#back-to-top').click(function () {
      $('#back-to-top').tooltip('hide');
      $('body,html').animate({ scrollTop: 0 }, 800); return false; });
    $('#back-to-top').tooltip('show');
  })
</script>


  
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">
  

  
    <link rel="shortcut icon" type="image/x-icon" href="https://takeawildguess.net/images/logo/twgLogo.png">
  

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  

  
  
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  

</head>

  <!-- Navigation -->
<nav class="navbar fixed-top navbar-expand-md navbar-dark bg-dark">
  
    <a class="navbar-brand abs" href="https://takeawildguess.net/">
      <img src="https://takeawildguess.net/images/logo/twgLogo.png" class="img-responsive" id="nav-logo" alt="How to learn to classify - Part 3">
    </a>
  
  <a class="navbar-brand" href="/blog/logisticregression_part3/" style="font-size: 16px; ">Is it coffee or cappuccino?</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsingNavbar">
      <span class="navbar-toggler-icon"></span>
  </button>
  <div class="navbar-collapse collapse" id="collapsingNavbar">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/">Home <span class="sr-only">(current)</span></a></li>
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/blog/">Blog</a></li>
        
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="https://takeawildguess.net/about/" id="navbarDropdown" role="button"
          data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            <a class="dropdown-item" href="https://takeawildguess.net/about/">Main</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_twg">TWG</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_me">Me</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_skl">Skills</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_exp">Experience</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_pt">Talks</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/resume/">Resume</a>
          </div>
        </li>

        
      </ul>
      
      <ul class="navbar-nav navbar-right">
        
          <li class="nav-item navbar-icon"><a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a></li>
        
      </ul>
      
  </div>
  <div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
  </div>
</nav>

  <body data-spy="scroll" data-target="#toc">
    <!-- Hero -->
<header class="postHead">
  <div class="container-fluid overlay">
    <div class="descr">
      <img src="https://takeawildguess.net/blog/images/coffee_cappuccino.jpeg" class="img-fluid" alt="__">
      <div class="card">
        <div class="card-body">
          <h1 class="card-title text-center">How to learn to classify - Part 3</h1>
          <h6 class="card-text text-center">February 17, 2019</h6>
          <h6 class="card-text text-center"><span class="fa fa-clock-o"></span> 16 min read</h6>
          <h6 class="card-text text-center">
            <a href="https://takeawildguess.net/tags/logistic-regression"><kbd class="item-tag">logistic-regression</kbd></a> <a href="https://takeawildguess.net/tags/algorithm"><kbd class="item-tag">algorithm</kbd></a> <a href="https://takeawildguess.net/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a> <a href="https://takeawildguess.net/tags/python"><kbd class="item-tag">python</kbd></a> <a href="https://takeawildguess.net/tags/scikit-learn"><kbd class="item-tag">scikit-learn</kbd></a> <a href="https://takeawildguess.net/tags/tensorflow"><kbd class="item-tag">tensorflow</kbd></a> 
          </h6>
        </div>
      </div>
    </div>
  </div>
</header>

    <section class="postContent">
  <div class="container">
    <div class="row">
      
      <div class="col-sm-2 col-lg-2">
        <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
      </div>
      
      <div class="col-lg-10 col-sm-10">
        <div class="container blogPost">
          

<h2 id="1-introduction-and-assumptions">1. Introduction and assumptions</h2>

<p>In this post-series, we are going to study the very basic modeling for classification problems, the logistic regression.
<strong>Classification</strong> entails that the output is a discrete variable taking values on a pre-defined limited set, where the set dimension is the number of classes. Some examples are <em>spam detection</em>, <em>object recognition</em> and <em>topic identification</em>.</p>

<p>We focus on the input/output space, the predictor structure, the learning algorithm and on applying the method to different datasets.
In this series, we do not split the dataset into training and testing sets, but we assess every model on the training set only.
A dedicated post on model selection, overfitting/underfitting problem and regularization will be published soon.</p>

<p>Here follows the post-series steps:</p>

<ol>
<li>Probabilistic model of a classification problem and cross-entropy definition via maximum likelihood (<a href="/blog/logisticregression_part1/">Part 1</a>).</li>
<li>Logistic regression developed from scratch with Python and Numpy (<a href="/blog/logisticregression_part1/">Part 1</a>).</li>
<li>Logistic regression implementation in Scikit-learn and TensorFlow (<a href="/blog/logisticregression_part2/">Part 2</a>).</li>
<li>How to model different predictor spaces, namely 1D, 2D and 3D (<a href="/blog/logisticregression_part2/">Part 2</a>).</li>
<li>Multinomial classification, where both softmax function and one-vs-all method are applied and compared (Part 3).</li>
<li>Non-linear input predictors (Part 3).</li>
<li>Categorical and numerical predictors (<a href="/blog/logisticregression_part4/">Part 4</a>).</li>
<li>Logistic regression applied to the <strong>digits</strong> dataset (<a href="/blog/logisticregression_part4/">Part 4</a>).</li>
<li>Logistic regression applied to the <strong>MNIST</strong> dataset (<a href="/blog/logisticregression_part4/">Part 4</a>).</li>
</ol>

<h2 id="2-binary-and-multinomial-classification">2. Binary and multinomial classification</h2>

<p>Binary classification entails that the outcome can belong to either one class or the other one, such as <em>spam detection</em>, where the outcome can belong to either the “spam” class or the “ham”.
To this end, we have introduced the sigmoid function that shrink the entire real domain to the (0, 1) interval, which is suitable to describe the probabilistic domain of one of the two classes.
The probability of the other one is then the complement to 1.
What does it happen when the outcome can belong to more than two classes?
We face the <strong>multinomial (or multi-class) classification</strong> problem.
There are two ways to solve this problem:
1. <strong>one-vs-all</strong>, where one logistic regression classifier for each of the K classes of the dataset. Each separate classifier will aim at identifying one specific class, for which it returns 1 while 0 otherwise. At prediction time, the class that receives the highest probability from the K classifiers is selected as the winner.
2. <strong>softmax</strong>, where entire probability distribution over the K classes is return as output of the model by means of the <em>softmax</em> function.</p>

<h3 id="2-1-data-generation">2.1 Data generation</h3>

<p>We create the dataset by applying the same procedure used for binary classification and combining the two distributions to get the response taking values from the set (0, 1, 2, 3).
In fact the first temporary variable is a dichotomous variable whose values belong to (0, 2) due to the 2 factor; the second temporary variable ranges in (1, 2) due to the 1 bias.
In that way, if we sum the two variables we can only get (1, 2) and (3, 4) values. The final $-1$ bias shifts the set to the desired one.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from mpl_toolkits import mplot3d

Nx, Ny = 30, 20
Npnt = 1000
xy = 5*(2*np.random.rand(Npnt, 2)-1)
xx1, xx2 = xy[:,0], xy[:,1]
w10, w11, w12 = 2, 3, -1
w20, w21, w22 = -2, -1, 2
noise = 0.05*(np.random.randn(Npnt)-1)
hh1 = w10 + w11*xx1 + w12*xx2 + noise
hh2 = w20 + w21*xx1 + w22*xx2 + noise
Y1noise = 1/(1+np.exp(-hh1))
Y2noise = 1/(1+np.exp(-hh2))
Y1cls = np.random.binomial(1., Y1noise)*2 # dichotomous variable, n_trial=1 =&gt; Bernoulli distribution
Y2cls = np.random.binomial(1., Y2noise)+1
Ycls = Y1cls + Y2cls - 1
XX = xy.copy()
Ycls = Ycls.flatten().reshape(-1,1)
print([XX.shape, Ycls.shape])
</code></pre>

<pre><code>[(1000, 2), (1000, 1)]
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;);
</code></pre>

<p><img src="/blog/logReg3/output_5_0.png" alt="png" /></p>

<h3 id="2-2-multinomial-classification">2.2 Multinomial classification</h3>

<p>In Sklearn, we need to change the solver to <em>sag</em> when we want define the classification task as multinomial, where the intermediate response variable is the probability distribution over the set of discrete classes.</p>

<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn import metrics
# multi-nomial
lgr = LogisticRegression(C=1e5, solver='sag', multi_class='multinomial') # we want to ignore regularization
YY = Ycls[:, 0]
lgr.fit(XX, YY)
Ypred = lgr.predict(XX)
Ypred_prob = lgr.predict_proba(XX)
Ypred.shape, Ypred_prob.shape
</code></pre>

<pre><code>((1000,), (1000, 4))
</code></pre>

<p>We need also to specify how the precision and recall can be calculated due to multiple classes.
In other words, the false-positive definition does not fit to the non-binary case per se.
Here we set the <em>average</em> attribute to <em>weighted</em>, so that it calculates the metrics for each label and finds their average weighted by support (the number of true instances for each label). See Sklearn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">doc</a>.</p>

<pre><code class="language-python">print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(Ycls, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(Ycls, Ypred, average='weighted')))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(Ycls, Ypred, average='weighted')))
</code></pre>

<pre><code>Accuracy: 0.905
Precision: 0.9047666713801842
Recall: 0.905
</code></pre>

<p>The size of the response domain (number of classes) is derived from the parameter size itself.
The coefficient 2D array has two columns as the number of predictors.</p>

<pre><code class="language-python">print(&quot;Intercept size: {}&quot;.format(lgr.intercept_.shape))
print(&quot;Weight array size: {}&quot;.format(lgr.coef_.shape))
</code></pre>

<pre><code>Intercept size: (4,)
Weight array size: (4, 2)
</code></pre>

<p>We define the two dimensional grid for the two predictors and determine the model predicted class by using the <em>predict</em> method.
The y grid is then reshaped to match the x1 grid size.</p>

<pre><code class="language-python">Npnt = 50  # number of points of the mesh
mrg = .5
x1min, x1max = xx1.min() - mrg, xx1.max() + mrg
x2min, x2max = xx2.min() - mrg, xx2.max() + mrg
x1grd, x2grd = np.meshgrid(np.linspace(x1min, x1max, Npnt), np.linspace(x2min, x2max, Npnt))
ygrd = lgr.predict(np.vstack((x1grd.ravel(), x2grd.ravel())).T)
ygrd = ygrd.reshape(x1grd.shape)

x1line = np.linspace(x1min, x1max, Npnt).reshape(-1, 1)
x2line = -(x1line*lgr.coef_[:,0] + lgr.intercept_)/lgr.coef_[:,1]
</code></pre>

<p>We want also to get the decision boundaries, one for each class.
The line equation is set with the x1 predictor as the input and the x2 predictor as the output.
Due to multiple parameter array, we obtain a 2 dimensional <em>x2line</em> array with 4 columns, one for each class.
Here the summary of the four decision boundaries:</p>

<ol>
<li>The blue dashed line delimits the Y=0 class (purple dots within the light blue area).</li>
<li>The orange dashed line delimits the Y=1 class (blue dots within the light green area).</li>
<li>The green dashed line delimits the Y=2 class (green dots within the light yellow area).</li>
<li>The red dashed line delimits the Y=3 class (yellow dots within the light brown area).</li>
</ol>

<p>The 0/3 class decision boundaries almost coincide each other, as much as the <sup>1</sup>&frasl;<sub>2</sub> class boundaries do.
This proves that the four classes have been generated by the combination of two simple Bernoulli distributions, each with a specific decision boundary.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
# contour
plt.contourf(x1grd, x2grd, ygrd, cmap=plt.cm.Paired, alpha=0.4)
plt.title(&quot;Decision surface of Multinomial classification with Logistic Regression&quot;)
plt.axis('tight')
# dataset
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
# decision boundary
plt.plot(x1line, x2line[:,:], ls=&quot;--&quot;);
</code></pre>

<p><img src="/blog/logReg3/output_16_0.png" alt="png" /></p>

<h3 id="2-3-one-vs-all-classification">2.3 One-vs-all classification</h3>

<p>The one-vs-all classification paradigm states the multi-class problem as multiple binary classification problems.
In other words, if we need to build a N-class classifier (with $N&gt;2$), we treat the process as follows:
1. we build N separate binary classifiers
2. we train the j-th classifier to classify class j as positive class and everything else as negative class.
3. at inference stage, we feed the input to every classifier and assign to the response the class j whose corresponding classifier has returned the highest probability.</p>

<p>In practice, it is accomplished by setting the <em>multi_class</em> attribute to <em>ovr</em>, which stands for <strong>One-Versus-Rest</strong>.</p>

<pre><code class="language-python"># one versus rest
lgr = LogisticRegression(C=1e5, solver='sag', multi_class='ovr') # we want to ignore regularization
YY = Ycls[:, 0]
lgr.fit(XX, YY)
Ypred = lgr.predict(XX)
Ypred_prob = lgr.predict_proba(XX)
Ypred.shape, Ypred_prob.shape
</code></pre>

<pre><code>((1000,), (1000, 4))
</code></pre>

<p>We apply the same code to get the model metrics.</p>

<pre><code class="language-python">print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred, average='weighted')))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred, average='weighted')))
</code></pre>

<pre><code>Accuracy: 0.893
Precision: 0.8941398474348617
Recall: 0.893
</code></pre>

<p>We want also to get the decision boundaries, one for each class.
Here the summary of the four decision boundaries:</p>

<ol>
<li>The blue dashed line delimits the Y=0 class (purple dots within the light blue area).</li>
<li>The orange dashed line delimits the Y=1 class (blue dots within the light green area).</li>
<li>The green dashed line delimits the Y=2 class (green dots within the light yellow area).</li>
<li>The red dashed line delimits the Y=3 class (yellow dots within the light brown area).</li>
</ol>

<p>The 0/3 class and the <sup>1</sup>&frasl;<sub>2</sub> class decision boundaries do no longer match each other.
Indeed, the decision boundary of class j is still &ldquo;parallel&rdquo; to the opposite class&rsquo;s boundary, but it is shifted towards the class centroid to separate its class to the rest as much as possible.</p>

<pre><code class="language-python">ygrd = lgr.predict(np.vstack((x1grd.ravel(), x2grd.ravel())).T)
ygrd = ygrd.reshape(x1grd.shape)

x1line = np.linspace(x1min, x1max, Npnt).reshape(-1, 1)
x2line = -(x1line*lgr.coef_[:,0] + lgr.intercept_)/lgr.coef_[:,1]

plt.figure(figsize=(10, 5))
# contour
plt.contourf(x1grd, x2grd, ygrd, cmap=plt.cm.Paired, alpha=0.4)
plt.title(&quot;Decision surface of LogisticRegression&quot;)
plt.axis('tight')
# dataset
#plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.scatter(xx1[YY==0], xx2[YY==0], cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
# decision boundary
plt.plot(x1line, x2line[:,0], ls=&quot;--&quot;);
</code></pre>

<p><img src="/blog/logReg3/output_23_0.png" alt="png" /></p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
# contour
plt.contourf(x1grd, x2grd, ygrd, cmap=plt.cm.Paired, alpha=0.4)
plt.title(&quot;Decision surface of LogisticRegression&quot;)
plt.axis('tight')
# dataset
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
# decision boundary
plt.plot(x1line, x2line, ls=&quot;--&quot;);
</code></pre>

<p><img src="/blog/logReg3/output_24_0.png" alt="png" /></p>

<h2 id="3-two-dimensional-non-linear-space-of-predictors">3. Two-dimensional non linear space of predictors</h2>

<p>Here we show how the logistic regression model handles <strong>non-linear</strong> 2D predictor space. However, the problem is still binary classification since the y response can only take 0 or 1 values.
A non-linear space implies that the decision boundary that separates the two classes, if exists, is not a line anymore.
In this section, we will face an example of circle-shaped boundary.</p>

<h3 id="3-1-two-dimensional-non-linear-data-generation">3.1 Two-dimensional non-linear data generation</h3>

<p>We create the Bernoulli distribution for the two predictors, x1 and x2, from two intermediate variables, $rr$ and $tt$, which model the radius and the angle of a point with respect to the <a href="https://en.wikipedia.org/wiki/Polar_coordinate_system">polar coordinate system</a>.
We apply the circle polar transformation to get the cartesian coordinates. However, the dichotomous response variable is controlled directly with the intermediate variables, so that the decision boundary should be:</p>

<p>$ hh = \omega_0 + \omega_1\cdot r + \omega_2\cdot \theta = 0 \Rightarrow r = -\omega_0/\omega_1 = 5 $</p>

<p>where $\omega_2$ has been set to 0.</p>

<pre><code class="language-python">Npntx, Npnty = 10, 50 # number of points
r_ = np.linspace(1, 10, Npntx)
t_ = np.linspace(0, 2*np.pi, Npnty)
rr, tt = np.meshgrid(r_, t_)
noise = 0*(np.random.randn(Npnty,Npntx)-1)
xx1 = rr*np.cos(tt)
xx2 = rr*np.sin(tt)
w0, w1, w2 = -5, 1, 0  # ground-truth parameters
hh = w0 + w1*rr + w2*tt + noise
Ynoise = 1/(1+np.exp(-hh))
Ycls = np.random.binomial(1., Ynoise) # dichotomous variable, n_trial=1 =&gt; Bernoulli distribution

plt.figure(figsize=(10, 5))
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;);
</code></pre>

<p><img src="/blog/logReg3/output_27_0.png" alt="png" /></p>

<pre><code class="language-python">XX = np.vstack((xx1.flatten(), xx2.flatten())).T
Ycls = Ycls.flatten().reshape(-1,1)
print([XX.shape, Ycls.shape])
</code></pre>

<pre><code>[(500, 2), (500, 1)]
</code></pre>

<h3 id="3-2-logistic-regression-of-linear-predictors-only">3.2 Logistic regression of linear predictors only</h3>

<p>We build the logistic regression model as a function of the two raw predictors only.
Since the two-class delimiter is not linear, it is not possible for this model to correctly classify this dataset.
The following steps want to prove this statement.
In the next sub-section we introduce the polynomial feature method to extend the input predictor space to non-linear features and test it to the dataset classification.</p>

<pre><code class="language-python">lgr = LogisticRegression(C=1e5) # we want to ignore regularization
YY = Ycls[:, 0]
lgr.fit(XX, YY)
Ypred = lgr.predict(XX)
Ypred_prob = lgr.predict_proba(XX)
Ypred.shape, Ypred_prob.shape
</code></pre>

<pre><code>((500,), (500, 2))
</code></pre>

<p>The accuracy and the precision highlights the poor performance of the model, which is as low as what we can get with the trivial model that returns 1 for any input.
The trivial model accuracy can be calculated as the fraction of instances of the Y variable being equal to 1, to the total number of instances.</p>

<pre><code class="language-python">print(&quot;Accuracy of the trivial model: {}&quot;.format(np.sum(YY==1)/YY.shape[0]))
print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred)))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred)))
</code></pre>

<pre><code>Accuracy of the trivial model: 0.55
Accuracy: 0.55
Precision: 0.55
Recall: 1.0
</code></pre>

<p>Figure confirms that the model is not able to split the two classes at all.</p>

<pre><code class="language-python">Npnt = 50  # number of points of the mesh
mrg = .5
x1min, x1max = xx1.min() - mrg, xx1.max() + mrg
x2min, x2max = xx2.min() - mrg, xx2.max() + mrg
x1grd, x2grd = np.meshgrid(np.linspace(x1min, x1max, Npnt), np.linspace(x2min, x2max, Npnt))
ygrd = lgr.predict(np.vstack((x1grd.ravel(), x2grd.ravel())).T)
ygrd = ygrd.reshape(x1grd.shape)

plt.figure(figsize=(10, 5))
# contour
plt.contourf(x1grd, x2grd, ygrd, cmap=plt.cm.Paired, alpha=0.4)
plt.title(&quot;Decision surface of Logistic Regression with linear features&quot;)
plt.axis('tight')
# dataset
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
# decision boundary
plt.contour(x1grd, x2grd, ygrd, levels = [0], colors=('k',), linestyles=('-',),linewidths=(2,));
</code></pre>

<p><img src="/blog/logReg3/output_34_0.png" alt="png" /></p>

<p>We introduce the Scikit-learn method to generate polynomial and interaction features, from a set of input features.
If we have two inputs, ($x_1$, $x_2$), we can get the full set of a polynomial class of degree N.
For instance, if n = 3, we get the following 10-sized set:</p>

<p>$ (1, x_1, x_2, x_1^2, x_1\cdot x_2, x_2^2, x_1^3, x_1^2\cdot x_2, x_1\cdot x_2^2, x_2^3) $</p>

<p>Here we show what we get if ($x_1$, $x_2$) = (3, 2).</p>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

pf = PolynomialFeatures(3)
ff = np.array([3, 2])
print(ff)
print(pf.fit_transform(ff.reshape(1,-1)))
</code></pre>

<pre><code>[3 2]
[[  1.   3.   2.   9.   6.   4.  27.  18.  12.   8.]]
</code></pre>

<pre><code class="language-python">pf = PolynomialFeatures(2)
XXnl = pf.fit_transform(XX) # not-linear
print(XXnl.shape)
</code></pre>

<pre><code>(500, 6)
</code></pre>

<pre><code class="language-python">lgr = LogisticRegression(C=1e5) # we want to ignore regularization
lgr.fit(XXnl, YY)
Ypred = lgr.predict(XXnl)
</code></pre>

<p>The model performance has improved very much.</p>

<pre><code class="language-python">print(&quot;predictor parameters: ({:.3f},{:.3f}), intercept of decision boundary: {:.3f}&quot;\
      .format(lgr.coef_[0,0], lgr.coef_[0,1], lgr.intercept_[0]))
print(&quot;Accuracy: {}&quot;.format(metrics.accuracy_score(YY, Ypred)))
print(&quot;Precision: {}&quot;.format(metrics.precision_score(YY, Ypred)))
print(&quot;Recall: {}&quot;.format(metrics.recall_score(YY, Ypred)))
</code></pre>

<pre><code>predictor parameters: (-1.229,0.016), intercept of decision boundary: -1.229
Accuracy: 0.842
Precision: 0.892
Recall: 0.8109090909090909
</code></pre>

<p>We show the new decision boundary, where brown is for yellow dots that belong to class 1, while cyan is for purple dots from class 0.
Since the decision boundary is not linear, it is not trivial to get its explicit or parametric formulation.
We exploit the contour function from matplotlib, which can draw isolines of a surface for specific z-levels.
In this case it is enough to set the <em>levels</em> attribute to the single 0-element list, in order to get the solid red line.</p>

<pre><code class="language-python">ygrd = lgr.predict(pf.fit_transform(np.vstack((x1grd.ravel(), x2grd.ravel())).T))
ygrd = ygrd.reshape(x1grd.shape)

plt.figure(figsize=(10, 5))
# contour
plt.contourf(x1grd, x2grd, ygrd, cmap=plt.cm.Paired, alpha=0.4)
plt.title(&quot;Decision surface of Logistic Regression with non-linear features&quot;)
plt.axis('tight')
# dataset
plt.scatter(xx1, xx2, c=Ycls, cmap='viridis')
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
# decision boundary
cs = plt.contour(x1grd, x2grd, ygrd, levels = [0], colors=('r',), linestyles=('-',),linewidths=(3,));
#plt.clabel(cs, fmt = '%2.1d', colors = 'k', fontsize=14) #contour line labels
</code></pre>

<p><img src="/blog/logReg3/output_43_0.png" alt="png" /></p>

<h2 id="reference">Reference</h2>

<ol>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">CS229 notes</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning at Coursera</a></li>
<li><a href="http://people.math.gatech.edu/~ecroot/3225/maximum_likelihood.pdf">Maximum likelihood estimators and least squares</a></li>
<li><a href="https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf">An Introduction to Statistical Learning</a></li>
<li><a href="https://stats.stackexchange.com/questions/46523/how-to-simulate-artificial-data-for-logistic-regression/46525">Generating artificial data</a></li>
<li><a href="https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">Logistic Regression overview</a></li>
<li><a href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/">Logistic Regression for Machine learning</a></li>
<li><a href="https://data.princeton.edu/wws509/notes/c3.pdf">Princeton notes</a></li>
<li><a href="https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8">Building A Logistic Regression in Python, Step by Step</a></li>
<li><a href="https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a">Logistic Regression using Python (scikit-learn)</a></li>
</ol>

        </div>
        <div class="pgNav PageNavigation col-12 text-center">
          <span>
          <p>&laquo; <a class="" href="/blog/logisticregression_part2/" style="color: #4ABDAC; font-size: 18px; "> How to learn to classify - Part 2</a>

          
          &nbsp;&nbsp; | &nbsp;&nbsp;
          <a class="" href="/blog/logisticregression_part4/" style="color: #4ABDAC; font-size: 18px; ">How to learn to classify - Part 4</a>
          
          &raquo;</p>
          </span>
          
        </div>

      </div>
    </div>
  </div>
</section>



    
    

    

    <footer class="pgFoot">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 text-center icons">
        
        <span>
          <a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a>
        </span>
        
      </div>

      <hr style="border: 2px solid #4ABDAC; min-width: 250px; border-radius: 2px; " />
      <div class="col-12 text-center">
        <p>
          &bull; Copyright &copy; 2019, <a href="https://takeawildguess.net/">Mattia Venditti</a> &bull; All rights reserved. &bull;
          <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
          </a>
          All blog posts are released under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </p>
      </div>
      

      <div class="col-6 text-center">
        <p>Disclaimer: The views and opinions on this website are my own and do not reflect or represent the views of my employer.</p>
      </div>
      <div class="col-6 text-center">
        <p>
        Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://pages.github.com/">GitHub Pages</a>.
        The favicon and logo were created by myself.
        </p>
      </div>

    </div>
  </div>
</footer>

<a id="back-to-top" href="https://takeawildguess.net/" class="btn btn-primary btn-lg back-to-top" role="button" title="Click to return on the top page"
data-toggle="tooltip" data-placement="left"><i class="fa fa-angle-up"></i></a>


  </body>
</html>
