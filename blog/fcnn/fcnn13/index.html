<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

   <meta name="description" content="Personal website"> 
   <meta name="author" content="Your name"> 
  

  <meta name="generator" content="Hugo 0.59.1" />
  <title>Multi-hyperparameter analysis of a neural network and computational comparison &middot; take a wild guess</title>

  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">



 <link rel="stylesheet" href="https://takeawildguess.net/css/main.css"> 


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css">

  
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>

<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>



    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script> 
    <script>hljs.initHighlightingOnLoad();</script>






<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>



<script type="text/javascript">
  window.onscroll = function() {myFunction()};
  function myFunction() {
    var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
    var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    var scrolled = (winScroll / height) * 100;
    document.getElementById("myBar").style.width = scrolled + "%";
  }
</script>


<script type="text/javascript">
  $(document).ready(function(){
    $(window).scroll(function () {
      if ($(this).scrollTop() > 50) { $('#back-to-top').fadeIn(); } else { $('#back-to-top').fadeOut(); }
    });
    
    $('#back-to-top').click(function () {
      $('#back-to-top').tooltip('hide');
      $('body,html').animate({ scrollTop: 0 }, 800); return false; });
    $('#back-to-top').tooltip('show');
  })
</script>


  
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">
  

  
    <link rel="shortcut icon" type="image/x-icon" href="https://takeawildguess.net/images/logo/twgLogo.png">
  

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  
    
    
    
    
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-151389132-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
  

  
  
  







<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']], displayMath: [['$$','$$']],
    processEscapes: true, processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }});
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

  

</head>

  <!-- Navigation -->
<nav class="navbar fixed-top navbar-expand-md navbar-dark bg-dark">
  
    <a class="navbar-brand abs" href="https://takeawildguess.net/">
      <img src="https://takeawildguess.net/images/logo/twgLogo.png" class="img-responsive" id="nav-logo" alt="Multi-hyperparameter analysis of a neural network and computational comparison">
    </a>
  
  <a class="navbar-brand" href="/blog/fcnn/fcnn13/" style="font-size: 16px; ">Meta-learning NNs</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsingNavbar">
      <span class="navbar-toggler-icon"></span>
  </button>
  <div class="navbar-collapse collapse" id="collapsingNavbar">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/">Home <span class="sr-only">(current)</span></a></li>
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/blog/">Blog</a></li>
        
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="https://takeawildguess.net/about/" id="navbarDropdown" role="button"
          data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            <a class="dropdown-item" href="https://takeawildguess.net/about/">Main</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_twg">TWG</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_me">Me</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_skl">Skills</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_exp">Experience</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_pt">Talks</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/resume/">Resume</a>
          </div>
        </li>

        
      </ul>
      
      <ul class="navbar-nav navbar-right">
        
          <li class="nav-item navbar-icon"><a href="https://github.com/takeawildguess/" target="_blank"><i class="fa fa-github"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://twitter.com/takeawildguess4/" target="_blank"><i class="fa fa-twitter"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
      </ul>
      
  </div>
  <div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
  </div>
</nav>

  <body data-spy="scroll" data-target="#toc">
    <!-- Hero -->
<header class="postHead">
  <div class="container-fluid overlay">
    <div class="descr">
      <img src="https://takeawildguess.net/blog/images/fcnnHead1.jpg" class="img-fluid" alt="__">
      <div class="card">
        <div class="card-body">
          <h1 class="card-title text-center">Multi-hyperparameter analysis of a neural network and computational comparison</h1>
          <h5 class="card-title text-center">Meta-learning NNs</h5>
          <h6 class="card-text text-center">November 17, 2019</h6>
          <h6 class="card-text text-center"><span class="fa fa-clock-o"></span> 8.5 min read</h6>
          <h6 class="card-text text-center">
            <a href="https://takeawildguess.net/tags/python"><kbd class="item-tag">python</kbd></a> <a href="https://takeawildguess.net/tags/neural-network"><kbd class="item-tag">neural network</kbd></a> <a href="https://takeawildguess.net/tags/meta-learning"><kbd class="item-tag">meta-learning</kbd></a> <a href="https://takeawildguess.net/tags/hyperparameter"><kbd class="item-tag">hyperparameter</kbd></a> 
          </h6>
        </div>
      </div>
    </div>
  </div>
</header>

    <section class="postContent">
  <div class="container">
    <div class="row">
      
      <div class="col-sm-2 col-lg-2">
        <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
      </div>
      
      <div class="col-lg-10 col-sm-10">
        <div class="container blogPost">
          

<h2 id="1-introduction">1. Introduction</h2>

<p>This post belongs to a new series of posts related to a huge and popular topic in machine learning: <strong>fully connected neural networks</strong>.</p>

<p>The general series scope is three-fold:</p>

<ol>
<li>visualize the model features and characteristics with schematic pictures and charts</li>
<li>learn to implement the model with different levels of abstraction, given by the framework used</li>
<li>have some fun with one of the hottest topics right now!</li>
</ol>

<p>In this new post we are going to:</p>

<ol>
<li>look for the best set of hyperparameters for each dataset type (regression and binary classification).</li>
<li>compare the computational effort of each of the three machine-learning libraries.</li>
</ol>

<p>The whole code to create a synthetic dataset and learn a neural network model with any of the four libraries mentioned above is wrapped into a Python class, <code>trainFCNN()</code>, and can be found in my Github <a href="https://github.com/takeawildguess/FCNNlib" target="_blank">repo</a>.</p>

<h2 id="2-the-best-set-of-hyperparameters-for-each-dataset-type">2. The best set of hyperparameters for each dataset type</h2>

<h3 id="2-1-regression">2.1 Regression</h3>

<p>Now we analysis only a few combinations of HP pairs for a regression problem, the sum of two squared inputs.
Here we create a new <code>tnn</code> instance with the <code>squares</code> dataset and display it with <code>plotPoints()</code>.
The colder the point colour is, the higher the corresponding value.</p>

<pre><code class="language-python">tnn = trainFCNN(nb_pnt=2500, dataset='squares')
tnn.plotPoints()
</code></pre>

<p><img src="output_3_0.png" alt="png" /></p>

<p>Here the list of HPs with the corresponding set of values used for the analysis, namely activation function, optimizer, learning rate, number of hidden layer neurons and network depth.</p>

<pre><code class="language-python">acts = ['sigmoid', 'relu'] # activations
opts = ['sgd', 'adam'] # optimizers
LRs = [1, 10] # learning rates
Nneurs = [4, 16] # number of hidden neurons
LLs = [1, 2] # depth
</code></pre>

<p>Since we are creating multiple models in succession, it is <a href="https://stackoverflow.com/questions/50895110/what-do-i-need-k-clear-session-and-del-model-for-keras-with-tensorflow-gpu" target="_blank">necessary</a> to clear the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session" target="_blank">session</a> when we restart the hyperparameter search cell, if we use Keras.</p>

<pre><code class="language-python">K.clear_session()
</code></pre>

<p>We train $2^5=32$ different models with Keras (<code>lib='ks'</code>) for 200 epochs.
The Python module <code>itertools.product</code> make it possible to use a single for loop to perform this 5-dimensional HP search.</p>

<pre><code class="language-python">mdls = []
for act, opt, LR, Nneur, LL in itertools.product(acts, opts, LRs, Nneurs, LLs):
    tnn.train(lib='ks', nb_epochs=200, dims=[Nneur]*LL, activation=act, opt=opt, lr=LR*1e-3)
    mdls.append(deepcopy(tnn))
</code></pre>

<p>We plot the loss history for each case.
There are 10 unique colours, while the marker changes every 10 curves.
We can appreciate how differently those models behave.
In particular, half of them can learn something meaningful, provided enough epochs, while the other half get stuck to the wrong point (in terms of parameter space).</p>

<pre><code class="language-python">colors = ['b', 'g', 'r', 'c', 'k']
markers = ['solid', 'dotted', 'dashed', 'dashdot']
descrKeys = ['units', 'act', 'opt', 'lr']
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(15, 12))
for kk, tnn in enumerate(mdls):
    mark = markers[kk // 10]
    plt.plot(tnn.lossHistory, label=tnn.mdlDescription(descrKeys), lw=2, ls=mark, alpha=.75)
plt.grid()
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('# of epochs')
plt.ylabel('model loss')
plt.show()
</code></pre>

<p><img src="output_12_0.png" alt="png" /></p>

<p>The fastest model comes with the largest network <code>2-16-16-4</code>, <code>relu</code> activation, <code>adam</code> optimizer and highest learning rate.
Its loss history is reported as a dash-dot orange line.</p>

<p>If the learning rate is reduced by an order of magnitude (from 1e-2 to 1e-3), the trend is slower (dashed gray line).</p>

<p>The second fastest model has one hidden layer only, <code>2-16-4</code>, (dash-dot blue line).</p>

<p>The impact of the number of hidden neurons is quite significant, as the dashed gold line highlights for the <code>2-4-4</code> network trained with the same settings as the benchmark model.</p>

<p>We report the model prediction using these two models.</p>

<pre><code class="language-python">mdls[-1].plotModelEstimate(figsize=(16, 9))
</code></pre>

<p><img src="output_14_0.png" alt="png" /></p>

<pre><code class="language-python">mdls[-4].plotModelEstimate(figsize=(16, 9))
</code></pre>

<p><img src="output_15_0.png" alt="png" /></p>

<p>We cannot appreciate any difference at all at first sight.
This chart is then useful to understand the model behaviour on a larger domain, but the loss chart is to be used for a proper hyperparameter selection.</p>

<p>Please be aware we are testing/validating each model on the whole dataset used to train the model itself.
The hyperparameters should be selected wrt a validation set that is distinct from the training one.
In this case, we are learning how to cope with toy examples.</p>

<h3 id="2-2-binary-classification">2.2 Binary classification</h3>

<p>Now we analysis only a few combinations of HP pairs for a binary classification problem, the <code>xor</code> function.</p>

<pre><code class="language-python">tnn = trainFCNN(nb_pnt=2500, dataset='xor')
tnn.plotPoints()
</code></pre>

<p><img src="output_18_0.png" alt="png" /></p>

<pre><code class="language-python">K.clear_session()
</code></pre>

<p>We train the same set of $2^5=32$ different models with Keras (<code>lib='ks'</code>) for 200 epochs.</p>

<pre><code class="language-python">mdls = []
for act, opt, LR, Nneur, LL in itertools.product(acts, opts, LRs, Nneurs, LLs):
    tnn.train(lib='ks', nb_epochs=200, dims=[Nneur]*LL, activation=act, opt=opt, lr=LR*1e-3)
    mdls.append(deepcopy(tnn))
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(15, 12))
for kk, tnn in enumerate(mdls):
    mark = markers[kk // 10]
    plt.plot(tnn.lossHistory, label=tnn.mdlDescription(descrKeys), lw=2, ls=mark, alpha=.75)
plt.grid()
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('# of epochs')
plt.ylabel('model loss')
plt.show()
</code></pre>

<p><img src="output_22_0.png" alt="png" /></p>

<p>The fastest model is still the one with the largest network <code>2-16-16-4</code>, <code>relu</code> activation, <code>adam</code> optimizer and highest learning rate (dash-dot orange line).</p>

<p>A fascinating case is <code>2-16-16-4/sigmoid/0.01/adam</code> (dotted brown line). The optimizer is able to get away from a non-optimal point it has been stuck for about 50 epochs and to massively reduce the loss in a few epochs.
It then converges to the same loss as the benchmark does.</p>

<p>The impact of the optimizer is quite significant, as the <code>2-16-16-4/sigmoid/0.01/sgd</code> case shows.
It cannot really improve that much wrt to the initial loss.
As shown earlier, the <code>sgd</code> optimizer needs proper settings for the learning rate.</p>

<p>We finally report the model prediction for the benchmark model and for its smaller version (<code>2-4-4</code>).</p>

<pre><code class="language-python">mdls[-1].plotModelEstimate(figsize=(16, 9))
</code></pre>

<p><img src="output_24_0.png" alt="png" /></p>

<pre><code class="language-python">mdls[-4].plotModelEstimate(figsize=(16, 9))
</code></pre>

<p><img src="output_25_0.png" alt="png" /></p>

<p>In this case, we can appreciate how the smaller network could not learn how to model the RHS domain.
Theoretically, we know it could learn it as well, so different settings are required.</p>

<p>You could give it a try. The code is there!</p>

<h2 id="3-comparing-the-computational-time-of-each-library">3. Comparing the computational time of each library</h2>

<p>We compare the three libraries, Keras, Tensorflow and Pytorch, to train the best-performing model, <code>2-16-16-4/relu/0.01/adam</code>, for 300 epochs.</p>

<p>Then we hugely increase the network size to merely test those libraries&rsquo; performance.</p>

<p>Timing comes out of a CPU run!</p>

<pre><code class="language-python">tnn = trainFCNN(nb_pnt=2500, dataset='squares')
</code></pre>

<h3 id="3-1-keras">3.1 Keras</h3>

<p>We start by clearing the session of Keras and estimating the computational time of each case with the Jupyter magic command <code>%%time</code>.</p>

<pre><code class="language-python">K.clear_session()
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='ks', nb_epochs=300, dims=[16]*2, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>Wall time: 5.65 s
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='ks', nb_epochs=300, dims=[64]*5, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>Wall time: 8.84 s
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='ks', nb_epochs=300, dims=[128]*5, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>Wall time: 24.1 s
</code></pre>

<h3 id="3-2-tensorflow">3.2 Tensorflow</h3>

<p>We move to Tensorflow.</p>

<pre><code class="language-python">%%time
tnn.train(lib='tf', nb_epochs=300, dims=[16]*2, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>The final model loss is 0.08039630949497223
Wall time: 4.1 s
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='tf', nb_epochs=300, dims=[64]*5, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>The final model loss is 0.1463933289051056
Wall time: 7.13 s
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='tf', nb_epochs=300, dims=[128]*5, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>The final model loss is 0.12102506309747696
Wall time: 17.4 s
</code></pre>

<h3 id="3-3-pytorch">3.3 Pytorch</h3>

<p>Let&rsquo;s test Pytorch.</p>

<pre><code class="language-python">%%time
tnn.train(lib='pt', nb_epochs=300, dims=[16]*2, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>Wall time: 6.41 s
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='pt', nb_epochs=300, dims=[64]*5, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>Wall time: 25.9 s
</code></pre>

<pre><code class="language-python">%%time
tnn.train(lib='pt', nb_epochs=300, dims=[128]*5, activation='relu', lr=.01, opt='adam')
</code></pre>

<pre><code>Wall time: 51.7 s
</code></pre>

<pre><code class="language-python">performances = [[5.65, 8.84, 24.1], [4.1, 7.13, 17.4], [6.4, 25.9, 51.7]]
sizes = [16*2, 64*5, 128*5]

plt.figure(figsize=(12, 6))
for perf, lib in zip(performances, ['keras', 'tensorflow', 'pytorch']):
    plt.plot(np.log(sizes), np.log(perf), label=lib, lw=2, alpha=.75)
plt.grid()
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('network size (log)')
plt.ylabel('computational time (log s)')
plt.show()
</code></pre>

<p><img src="output_42_0.png" alt="png" /></p>

<p>Tensorflow looks to be faster than the two competitors!
The strange thing is how Pytorch performance scales to increasing network sizes.
We need to investigate whether this issue is related to the code or to the specific configuration it is running on.
There will be a post on this topic!</p>

<p>If you want to start reading something about speeding up the code, you can check <a href="https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051" target="_blank">this post</a> out.
However, it mainly focuses on larger applications that fit fine to GPUs.</p>

        </div>
        <div class="pgNav PageNavigation col-12 text-center">
          <span>
          <p>&laquo; <a class="" href="/blog/fcnn/fcnn12/" style="color: #4ABDAC; font-size: 18px; "> Hyperparameter analysis for regression</a>
          
          &nbsp;&nbsp; | &nbsp;&nbsp;
          <a class="" href="/blog/fcnn/fcnn14/" style="color: #4ABDAC; font-size: 18px; ">Can we see inside a neural network?</a>
          
          &raquo;</p>
          </span>
          
          
        </div>
      </div>
    </div>
  </div>
</section>

    <div class="article-container">
      <script src="https://utteranc.es/client.js"
        repo="takeawildguess/pws"
        issue-term="pathname"
        label="Comment"
        theme="github-dark-orange"
        crossorigin="anonymous"
        async>
</script>

    </div>

    
    

    



    <footer class="pgFoot">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 text-center icons">
        
        <span>
          <a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a>
        </span>
        
      </div>

      <hr style="border: 2px solid #4ABDAC; min-width: 250px; border-radius: 2px; " />
      <div class="col-12 text-center">
        <p>
          &bull; Copyright &copy; 2019, <a href="https://takeawildguess.net/">Mattia Venditti</a> &bull; All rights reserved. &bull;
          <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
          </a>
          All blog posts are released under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </p>
      </div>
      

      <div class="col-6 text-center">
        <p>Disclaimer: The views and opinions on this website are my own and do not reflect or represent the views of my employer.</p>
      </div>
      <div class="col-6 text-center">
        <p>
        Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://pages.github.com/">GitHub Pages</a>.
        The favicon and logo were created by myself.
        </p>
      </div>

    </div>
  </div>
</footer>

<a id="back-to-top" href="https://takeawildguess.net/" class="btn btn-primary btn-lg back-to-top" role="button" title="Click to return on the top page"
data-toggle="tooltip" data-placement="left"><i class="fa fa-angle-up"></i></a>


  </body>
</html>
