<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

   <meta name="description" content="Personal website"> 
   <meta name="author" content="Your name"> 
  

  <meta name="generator" content="Hugo 0.49" />
  <title>Hello world example for Machine learning - Part 2 &middot; take a wild guess</title>

  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">


 <link rel="stylesheet" href="https://takeawildguess.net/css/main.css"> 


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.css">

  
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>



    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script> 
    <script>hljs.initHighlightingOnLoad();</script>






<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>



<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.js"></script>


<script type="text/javascript">
  window.onscroll = function() {myFunction()};
  function myFunction() {
    var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
    var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    var scrolled = (winScroll / height) * 100;
    document.getElementById("myBar").style.width = scrolled + "%";
  }
</script>


<script type="text/javascript">
  $(document).ready(function(){
    $(window).scroll(function () {
      if ($(this).scrollTop() > 50) { $('#back-to-top').fadeIn(); } else { $('#back-to-top').fadeOut(); }
    });
    
    $('#back-to-top').click(function () {
      $('#back-to-top').tooltip('hide');
      $('body,html').animate({ scrollTop: 0 }, 800); return false; });
    $('#back-to-top').tooltip('show');
  })
</script>


  
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">
  

  
    <link rel="shortcut icon" type="image/x-icon" href="https://takeawildguess.net/images/logo/twgLogo.png">
  

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  

  
  
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  

</head>

  <!-- Navigation -->
<nav class="navbar fixed-top navbar-expand-md navbar-dark bg-dark">
  
    <a class="navbar-brand abs" href="https://takeawildguess.net/">
      <img src="https://takeawildguess.net/images/logo/twgLogo.png" class="img-responsive" id="nav-logo" alt="Hello world example for Machine learning - Part 2">
    </a>
  
  <a class="navbar-brand" href="/blog/linearregression_part2/" style="font-size: 16px; ">Hello world</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsingNavbar">
      <span class="navbar-toggler-icon"></span>
  </button>
  <div class="navbar-collapse collapse" id="collapsingNavbar">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/">Home <span class="sr-only">(current)</span></a></li>
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/blog/">Blog</a></li>
        
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="https://takeawildguess.net/about/" id="navbarDropdown" role="button"
          data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            <a class="dropdown-item" href="https://takeawildguess.net/about/">Main</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_twg">TWG</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_me">Me</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_skl">Skills</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_exp">Experience</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_pt">Talks</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/resume/">Resume</a>
          </div>
        </li>

        
      </ul>
      
      <ul class="navbar-nav navbar-right">
        
          <li class="nav-item navbar-icon"><a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a></li>
        
      </ul>
      
  </div>
  <div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
  </div>
</nav>

  <body data-spy="scroll" data-target="#toc">
    <!-- Hero -->
<header class="postHead">
  <div class="container-fluid overlay">
    <div class="descr">
      <img src="https://takeawildguess.net/images/blog/linearRegression/hello.jpeg" class="img-fluid" alt="__">
      <div class="card">
        <div class="card-body">
          <h1 class="card-title text-center">Hello world example for Machine learning - Part 2</h1>
          <h5 class="card-title text-center">Hello world</h5>
          <h6 class="card-text text-center">January 13, 2019</h6>
          <h6 class="card-text text-center"><span class="fa fa-clock-o"></span> 13 min read</h6>
          <h6 class="card-text text-center">
            <a href="https://takeawildguess.net/tags/linear-regression"><kbd class="item-tag">linear-regression</kbd></a> <a href="https://takeawildguess.net/tags/algorithm"><kbd class="item-tag">algorithm</kbd></a> <a href="https://takeawildguess.net/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a> <a href="https://takeawildguess.net/tags/python"><kbd class="item-tag">python</kbd></a> <a href="https://takeawildguess.net/tags/numpy"><kbd class="item-tag">numpy</kbd></a> <a href="https://takeawildguess.net/tags/gradient-descent"><kbd class="item-tag">gradient descent</kbd></a> 
          </h6>
        </div>
      </div>
    </div>
  </div>
</header>

    <section class="postContent">
  <div class="container">
    <div class="row">
      
      <div class="col-sm-2 col-lg-2">
        <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
      </div>
      
      <div class="col-lg-10 col-sm-10">
        <div class="container blogPost">
          

<h2 id="1-introduction">1. Introduction</h2>

<p>We have introduced in the <a href="/blog/linearregression_part1/">previous</a> post the concept of the linear-regression problem and the structure to solve it in a &ldquo;machine-learning&rdquo; fashion.
In this post we apply the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way.</p>

<p>We are going to implement the logic from scratch in Python and its powerful numerical library, <a href="http://www.numpy.org/">Numpy</a>, by following the procedure illustrated in Fig. 1.</p>

<p><img src="/blog/linReg2/learningFramework.png" alt="png" title="Machine learning general structure" />
<em>Figure 1 - Machine learning general structure</em></p>

<h2 id="2-data-generation">2. Data generation</h2>

<p>First we start generating some synthetic data ($ N_{pnts} = 500 $ points). We assume we know both the slope ($ m = 5$) and the intercept ($ q = -3$) of the line we want to indentify, but we also introduce some noise with a gaussian distribution and zero-mean to the line to make the data source a bit closer to real-world scenarios.</p>

<p>The chart shows the generated data cloud that we feed to the learning algorithm to identify the line specifications.</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
Npnts = 500 # number of points
xx = np.linspace(0,3,Npnts)
noise = 0.5*(np.random.randn(Npnts)-1)
mm, qq = 5, -3
yy = mm*xx + qq + noise

plt.figure(figsize=(10, 5))
plt.scatter(xx, yy, alpha=0.75)
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
#plt.axis('equal')
plt.show()
</code></pre>

<p><img src="/blog/linReg2/output_3_0.png" alt="png" /></p>

<p>The dataset is generated by creating two 2D arrays, one for inputs and one for output.
The input array, XX, is the horizontal concatenation of a column of 1s, as many as the length of the initial 1D array <em>xx</em>, and the array itself. We first stack the two 1D arrays vertically and then transpose it to get the examples (500) over the rows and the features over the columns (2). The output 2D array is just a single column filled with the <em>y</em> values.
Here the shape of the arrays.</p>

<pre><code class="language-python">XX = np.vstack((np.ones_like(xx), xx)).T
YY = yy.reshape(-1,1)
[XX.shape, YY.shape]
</code></pre>

<pre><code>[(500, 2), (500, 1)]
</code></pre>

<pre><code class="language-python">XX = np.vstack((np.ones_like(xx), xx)).T
ww = np.array([0, -4]).reshape(-1, 1)
YY = yy.reshape(-1,1)
</code></pre>

<h2 id="3-loss-function">3. Loss function</h2>

<p>Let us recall the <a href="/blog/linearregression_part1/#lossFun">loss function</a>, <em>J</em>, where $x^j$ and $y^j$ are the input and output of the <em>j-th</em> example and <em>n</em> is the number of features (2):</p>

<p>$ J = \frac{1}{2}\sum_{j=1}^{m} (y^j- \sum_{k=1}^{n} (\theta_k \cdot x^j_k) )^2 $</p>

<p>The model parameters (or weights), $\theta$, are combined with the j-th example input, $x^j$, to return the predicted output.
It is then compared to the actual output, $y^j$, in terms of squared difference.
The following code gives the error of the j-th example, where the first method uses for-loop while the second one employs matrix multiplication.
The first method is chosen for exemplification, the second one is chosen for efficiency and therefore adopted in real implementation.</p>

<pre><code class="language-python">Nex, Nfeat = XX.shape # examples, features
ww = np.array([0, -4]).reshape(-1, 1)

jj = np.random.randint(Nex) # pick a random example

y_ = 0
for kk in range(Nfeat):
    y_ += ww[kk,0]*XX[jj, kk]

(YY[jj, 0] - y_)**2
</code></pre>

<pre><code>76.67599552148954
</code></pre>

<pre><code class="language-python">(np.dot(XX[jj,:], ww[:,0]) - YY[jj,0])**2
</code></pre>

<pre><code>76.67599552148954
</code></pre>

<p>If the full matrix product between input and parameter arrays is taken, we obtain a 2D array with a single column that stores the predictions over the set of examples. Its shape is identical to that of the actual ouput, <em>y</em>.
The overall loss is the sum of every error contribution, over the row axis of the examples, which is indexed with 0 in Numpy.
The final loss function implementation is as simple as what follows.</p>

<pre><code class="language-python">def lossFunction(XX, YY, ww):
    Npnt = XX.shape[0]
    J = np.sum((np.dot(XX, ww) - YY)**2, axis=0)/2/Npnt
    return J

lossFunction(XX, YY, ww)
</code></pre>

<pre><code>array([ 80.93621425])
</code></pre>

<h2 id="4-gradient-descent">4.Gradient descent</h2>

<p>We need to calculate the gradient of the loss function to apply the <em>gradient descent algorithm</em> to the linear regression problem.
We calculate the <em>k</em> element of the gradient:</p>

<p>$\frac{\partial J}{\partial \theta_k} = \frac{\partial}{\partial \theta_k}\frac{1}{2}\sum_{j=1}^{m} (y^j-\theta^T \cdot x^j)^2 $</p>

<p>$ = \frac{1}{2}\sum_{j=1}^{m} \frac{\partial}{\partial \theta_k}(y^j-\theta^T \cdot x^j)^2 $</p>

<p>$ = 2\frac{1}{2}\sum_{j=1}^{m} (y^j-\theta^T \cdot x^j)\frac{\partial}{\partial \theta_k}(y^j-\theta^T \cdot x^j) $</p>

<p>$ = \sum_{j=1}^{m} (y^j-\theta^T \cdot x^j)\cdot x_k^j $ (1)</p>

<p>Since the vector format of the linear regression expands to a sum as:</p>

<p>$ \theta^T \cdot x^j = \sum_{k=0}^n \theta_k\cdot x_k^j$</p>

<p>the derivative of this vector product with respect to the parameter $\theta_k$ is $x_k^j$.</p>

<p>Keep in mind that the input feature associated to the first parameter $\theta_0$, which is the intercept, is always 1.</p>

<p>Code-wise, a vectorized implementation of the parameter update step is fundamental to achieve fast computation.
The input data are stored into a 2D array, <em>XX</em>, where m and n+1 are number of rows and columns, respectively.
In the below code example, $m=6$ and $n=1$.
Parameters are allocated into a 2D array, <em>ww</em>, with n+1 rows and one column. Numpy <em>dot</em> operator returns the matrix multiplication of the two 2D arrays, whose size is (m, 1).
We then subtract element-wise the result with the ground-truth output, y, and broadcast the resulting (m, 1) array, $\epsilon$, to perform the element-wise product with the input array, XX.</p>

<pre><code class="language-python">X_ = np.arange(12).reshape(-1,2)
Y_ = np.arange(6).reshape(-1,1)
w_ = np.arange(2).reshape(-1,1)
epsilon = np.dot(X_, w_) - Y_
print(X_)
print('-'*10)
print(w_)
print('-'*10)
print(epsilon)
</code></pre>

<pre><code>[[ 0  1]
 [ 2  3]
 [ 4  5]
 [ 6  7]
 [ 8  9]
 [10 11]]
----------
[[0]
 [1]]
----------
[[1]
 [2]
 [3]
 [4]
 [5]
 [6]]
</code></pre>

<p>Broadcasting is a powerful Numpy feature that enable the element-wise product of two arrays that share at least one dimension (row or column), but the size of the other dimension of one of two arrays is equal to 1.
Two cases can occur that both lead to a (n, m) array:
1. A (n,m) array with a (n,1) array, where the second array is duplicated column-wise m times
2. A (n,m) array with a (1,m) array, where the second array is duplicated row-wise n times</p>

<p>The (m,1) array, $\epsilon$, represent the constant part for each parameter $\theta_k$. Since Eq. (1) states that the gradient element for $\theta_k$ is related to its corresponding feature $x_k$, we use the complete <em>XX</em> array to simultaneously update the entire parameter vector $\theta$.</p>

<pre><code class="language-python">print(epsilon.shape)
print(X_.shape)
print(epsilon * X_)
print((epsilon * X_).shape)
</code></pre>

<pre><code>(6, 1)
(6, 2)
[[ 0  1]
 [ 4  6]
 [12 15]
 [24 28]
 [40 45]
 [60 66]]
(6, 2)
</code></pre>

<p>The <em>sum</em> operator is Eq. (1) is active through the examples, which translates to the Numpy <em>sum</em> operator over the 0-axis. However, this step returns a 1D array, with size equal to the number of parameters to learn (n+1).
We need first <em>reshape</em> it to have a 2D column array (n+1, 1) and apply the update step with learning rate and actual parameter vector estimate.</p>

<pre><code class="language-python">grad_ = np.sum((np.dot(X_, w_) - Y_) * X_, axis=0)
print(grad_.shape)
print('-'*10)
print(grad_.reshape(-1,1))
</code></pre>

<pre><code>(2,)
----------
[[140]
 [161]]
</code></pre>

<h2 id="4-training">4. Training</h2>

<p>Here follows the complete code of the learning process, where the model parameters are changed for $N_{epoch}$ times.
The parameters and the corresponding loss function value are stored in associated lists and returned at the end of the function call.</p>

<pre><code class="language-python">def gradientDescent(XX, YY, ww, lr=0.1, Nepoch=1500):
    Npnt = XX.shape[0]
    Jevol, wevol = [], []
    for _ in range(Nepoch):
        Jevol.append(lossFunction(XX, YY, ww))
        wevol.append(ww[:,0])
        ww = ww - lr/Npnt * np.sum((np.dot(XX, ww) - YY) * XX, axis=0).reshape(-1,1)

    return np.array(wevol), np.array(Jevol)
</code></pre>

<p><em>wOpt</em> and <em>Jopt</em> are the optimal parameter set of values and the minimum loss that are stacked at the bottom of the evolution lists. The final values for <em>wOpt</em> are very closed to the ones used to generate the data.</p>

<pre><code class="language-python">J = lossFunction(XX, YY, ww)
Nepoch, lr = 1500, 0.05
wEvol, Jevol = gradientDescent(XX, YY, ww, lr, Nepoch)
wOpt, Jopt = wEvol[-1,:], Jevol[-1]
print(wOpt)
</code></pre>

<pre><code>[-3.40781857  4.97147063]
</code></pre>

<p>The below figure compares the line with optimal parameters, <em>wOpt</em>, to the cloud of points used to train the model.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.scatter(xx, yy, alpha=0.75)
plt.plot(xx, wOpt[0] + wOpt[1]*xx, 'r', lw=2)
plt.xlabel(&quot;X&quot;)
plt.ylabel(&quot;Y&quot;)
plt.show()
</code></pre>

<p><img src="/blog/linReg2/output_25_0.png" alt="png" /></p>

<h2 id="5-parameter-evolution">5. Parameter evolution</h2>

<p>Here we plot the evolution of the parameters $\omega$ over the $N_{epoch}$ training steps. The initial and final values are depicted with the green and red points, respectively. The size of each intermediate blue point is proportional to the loss function, where the smaller the point, the lower the loss, the better the model performance.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.scatter(wEvol[:,0], wEvol[:,1], s=10+100*Jevol/np.max(Jevol), alpha=0.5, label='Weight evolution')
plt.plot(wEvol[0,0], wEvol[0,1], 'g', linestyle='none', marker='o', markersize=10, alpha=0.75, label='Initial weights')
plt.plot(wEvol[-1,0], wEvol[-1,1], 'r', linestyle='none', marker='o', markersize=10, alpha=0.75, label='Optimal weights')
plt.xlabel(&quot;$\omega_1$&quot;)
plt.ylabel(&quot;$\omega_2$&quot;)
plt.legend()
plt.show()
</code></pre>

<p><img src="/blog/linReg2/output_27_0.png" alt="png" /></p>

<p>In the following plot, we compare the evolution of the parameters $\omega$ with the loss function 2D map.
To this end, we need to create a grid for every combination of $\omega_1$ and $\omega_2$, using <em>meshgrid</em>. The two 2D arrays are then flattened and veritcally stacked to obtain the parameter meshgrid, <em>wmg</em>, which is contains $20*26=520$ two-element tuples. We want to calculate the loss function for each pair of parameters.
The loss function requires a (2,1) array, so we need to reshape the 1D array that we get from for-iterating along the <em>wmg</em> rows.
The Python list comprehension helps performing this process that returns a list of 520 loss values corresponding to every point of the meshgrid. The final step is to convert the list into an array and reshape it into a 2D whose size is as equal as the initial 2D parameter arrays, <em>w1mg</em> and <em>w2mg</em>.
The plot shows the parameter evolution on top of the countor plot of the 2D loss function, where the greater the loss value the warmer the color.</p>

<pre><code class="language-python">step = 0.5
w1s = np.arange(-7, 3, step)
w2s = np.arange(-5, 8, step)
w1mg, w2mg = np.meshgrid(w1s, w2s)
wmg = np.vstack((w1mg.flatten(), w2mg.flatten())).T
w1s.shape, w2s.shape, wmg.shape
</code></pre>

<pre><code>((20,), (26,), (520, 2))
</code></pre>

<pre><code class="language-python">w_ = np.array([-3.5, 5]).reshape(-1, 1)
[w_, lossFunction(XX, YY, w_)]
</code></pre>

<pre><code>[array([[-3.5],
        [ 5. ]]), array([ 0.1389385])]
</code></pre>

<pre><code class="language-python">Jlist = [lossFunction(XX, YY, wmg[kk,:].reshape(-1, 1)) for kk in range(wmg.shape[0])]
Jmap = np.array(Jlist).reshape(-1, w1s.shape[0])
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.contourf(w1mg, w2mg, Jmap, alpha=0.5, cmap=plt.cm.jet)
plt.colorbar()
plt.scatter(wEvol[:,0], wEvol[:,1], s=10+100*Jevol/np.max(Jevol), alpha=0.5, label='Weight evolution')
plt.plot(wEvol[-1,0], wEvol[-1,1], 'r', linestyle='none', marker='o', markersize=10, alpha=0.75, label='Optimal weights')
plt.plot(wEvol[0,0], wEvol[0,1], 'g', linestyle='none', marker='o', markersize=10, alpha=0.75, label='Initial weights')
plt.xlabel(&quot;$\omega_1$&quot;)
plt.ylabel(&quot;$\omega_2$&quot;)
#plt.axis('equal')
plt.show()
</code></pre>

<p><img src="/blog/linReg2/output_32_0.png" alt="png" /></p>

<p>This plot reports the logarithmic trend of the loss function over the training steps.</p>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
plt.plot(np.log(Jevol), lw=2)
plt.xlabel(&quot;training steps ($N_{epoch}$)&quot;)
plt.ylabel(&quot;Logarithm loss trend ($log(J_{evol})$)&quot;)
plt.show()
</code></pre>

<p><img src="/blog/linReg2/output_34_0.png" alt="png" /></p>

<p>The final plot wants to summarize the post by visualizing the training process at specific steps, at which the logarithm of the loss function assume integer values. We create a list, <em>idxs</em>, that stores the $N_{row} = 6+1$ indices of the <em>Jevol</em> array that correspond to such training steps, in addition to the final training step.</p>

<pre><code class="language-python">idxs = [np.where(np.log(Jevol)&lt;kk)[0][0] for kk in range(4, -2, -1)] + [Jevol.shape[0]-1]
Nrow = len(idxs)
Nstep = int(Nepoch/Nrow)
print(Nrow)
</code></pre>

<pre><code>7
</code></pre>

<p>The chart has a matrix structure, with as many rows as the training steps to analyse, $N_{row}$, and two columns, the left-most one that compares the data point cloud to the current model (red line) and highlights the current loss (J value), the right-most one that shows the trajectory of the model parameters up to the current epoch within the 2D parameter space.</p>

<pre><code class="language-python">plt.figure(figsize=(15, Nrow*5))
for kk, idx in enumerate(idxs):
    plt.subplot(Nrow, 2, 2*kk+1)
    plt.scatter(xx, yy, alpha=0.75)
    plt.plot(xx, wEvol[idx,0] + wEvol[idx,1]*xx, 'r', lw=2)
    plt.xlabel(&quot;X&quot;)
    plt.ylabel(&quot;Y&quot;)
    plt.title(&quot;Current J value: {0:.2f}&quot;.format(Jevol[idx,0]))    

    plt.subplot(Nrow, 2, 2*kk+2)
    plt.contourf(w1mg, w2mg, Jmap, alpha=0.5, cmap=plt.cm.jet)
    plt.colorbar()
    plt.scatter(wEvol[:idx,0], wEvol[:idx,1], s=10+100*Jevol/np.max(Jevol), alpha=0.5, label='Weight evolution')
    plt.plot(wEvol[idx,0], wEvol[idx,1], 'r', linestyle='none', marker='o', markersize=10, alpha=0.75, label='Current weights')
    plt.xlabel(&quot;$\omega_1$&quot;)
    plt.ylabel(&quot;$\omega_2$&quot;)
    plt.title(&quot;Current epoch: &quot; + str(idx))

#plt.axis('equal')
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg2/output_38_0.png" alt="png" /></p>

<h2 id="reference">Reference</h2>

<ol>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">CS229 notes</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning at Coursera</a></li>
<li><a href="http://people.math.gatech.edu/~ecroot/3225/maximum_likelihood.pdf">Maximum likelihood estimators and least squares</a></li>
<li><a href="https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf">An Introduction to Statistical Learning</a></li>
</ol>

        </div>
        <div class="pgNav PageNavigation col-12 text-center">
          <span>
          <p>&laquo; <a class="" href="/blog/linearregression_part1/" style="color: #4ABDAC; font-size: 18px; "> Hello world example for Machine learning - Part 1</a>

          
          &nbsp;&nbsp; | &nbsp;&nbsp;
          <a class="" href="/blog/linearregression_part3/" style="color: #4ABDAC; font-size: 18px; ">Hello world example for Machine learning - Part 3</a>
          
          &raquo;</p>
          </span>
          
        </div>

      </div>
    </div>
  </div>
</section>



    
    

    

    <footer class="pgFoot">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 text-center icons">
        
        <span>
          <a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a>
        </span>
        
      </div>

      <hr style="border: 2px solid #4ABDAC; min-width: 250px; border-radius: 2px; " />
      <div class="col-12 text-center">
        <p>
          &bull; Copyright &copy; 2019, <a href="https://takeawildguess.net/">Mattia Venditti</a> &bull; All rights reserved. &bull;
          <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
          </a>
          All blog posts are released under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </p>
      </div>
      

      <div class="col-6 text-center">
        <p>Disclaimer: The views and opinions on this website are my own and do not reflect or represent the views of my employer.</p>
      </div>
      <div class="col-6 text-center">
        <p>
        Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://pages.github.com/">GitHub Pages</a>.
        The favicon and logo were created by myself.
        </p>
      </div>

    </div>
  </div>
</footer>

<a id="back-to-top" href="https://takeawildguess.net/" class="btn btn-primary btn-lg back-to-top" role="button" title="Click to return on the top page"
data-toggle="tooltip" data-placement="left"><i class="fa fa-angle-up"></i></a>


  </body>
</html>
