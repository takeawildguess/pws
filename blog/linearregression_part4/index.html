<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

   <meta name="description" content="Personal website"> 
   <meta name="author" content="Your name"> 
  

  <meta name="generator" content="Hugo 0.49" />
  <title>Hello world example for Machine learning - Part 4 &middot; take a wild guess</title>

  
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">


 <link rel="stylesheet" href="https://takeawildguess.net/css/main.css"> 


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.css">

  
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>



    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script> 
    <script>hljs.initHighlightingOnLoad();</script>






<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>



<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.0/dist/bootstrap-toc.min.js"></script>


<script type="text/javascript">
  window.onscroll = function() {myFunction()};
  function myFunction() {
    var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
    var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
    var scrolled = (winScroll / height) * 100;
    document.getElementById("myBar").style.width = scrolled + "%";
  }
</script>


<script type="text/javascript">
  $(document).ready(function(){
    $(window).scroll(function () {
      if ($(this).scrollTop() > 50) { $('#back-to-top').fadeIn(); } else { $('#back-to-top').fadeOut(); }
    });
    
    $('#back-to-top').click(function () {
      $('#back-to-top').tooltip('hide');
      $('body,html').animate({ scrollTop: 0 }, 800); return false; });
    $('#back-to-top').tooltip('show');
  })
</script>


  
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Kaushan+Script" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css">
  

  
    <link rel="shortcut icon" type="image/x-icon" href="https://takeawildguess.net/images/logo/twgLogo.png">
  

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  

  
  
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  

</head>

  <!-- Navigation -->
<nav class="navbar fixed-top navbar-expand-md navbar-dark bg-dark">
  
    <a class="navbar-brand abs" href="https://takeawildguess.net/">
      <img src="https://takeawildguess.net/images/logo/twgLogo.png" class="img-responsive" id="nav-logo" alt="Hello world example for Machine learning - Part 4">
    </a>
  
  <a class="navbar-brand" href="/blog/linearregression_part4/" style="font-size: 16px; ">Hello world</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsingNavbar">
      <span class="navbar-toggler-icon"></span>
  </button>
  <div class="navbar-collapse collapse" id="collapsingNavbar">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/">Home <span class="sr-only">(current)</span></a></li>
        <li class="nav-item"><a class="nav-link" href="https://takeawildguess.net/blog/">Blog</a></li>
        
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="https://takeawildguess.net/about/" id="navbarDropdown" role="button"
          data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            <a class="dropdown-item" href="https://takeawildguess.net/about/">Main</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_twg">TWG</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_me">Me</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_skl">Skills</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_exp">Experience</a>
            <a class="dropdown-item" href="https://takeawildguess.net/about/#a_pt">Talks</a>
            <div class="dropdown-divider"></div>
            <a class="dropdown-item" href="https://takeawildguess.net/resume/">Resume</a>
          </div>
        </li>

        
      </ul>
      
      <ul class="navbar-nav navbar-right">
        
          <li class="nav-item navbar-icon"><a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a></li>
        
          <li class="nav-item navbar-icon"><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a></li>
        
      </ul>
      
  </div>
  <div class="progress-container">
    <div class="progress-bar" id="myBar"></div>
  </div>
</nav>

  <body data-spy="scroll" data-target="#toc">
    <!-- Hero -->
<header class="postHead">
  <div class="container-fluid overlay">
    <div class="descr">
      <img src="https://takeawildguess.net/images/blog/linearRegression/hello.jpeg" class="img-fluid" alt="__">
      <div class="card">
        <div class="card-body">
          <h1 class="card-title text-center">Hello world example for Machine learning - Part 4</h1>
          <h6 class="card-text text-center">January 27, 2019</h6>
          <h6 class="card-text text-center"><span class="fa fa-clock-o"></span> 22 min read</h6>
          <h6 class="card-text text-center">
            <a href="https://takeawildguess.net/tags/linear-regression"><kbd class="item-tag">linear-regression</kbd></a> <a href="https://takeawildguess.net/tags/algorithm"><kbd class="item-tag">algorithm</kbd></a> <a href="https://takeawildguess.net/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a> <a href="https://takeawildguess.net/tags/python"><kbd class="item-tag">python</kbd></a> <a href="https://takeawildguess.net/tags/scikit-learn"><kbd class="item-tag">scikit-learn</kbd></a> <a href="https://takeawildguess.net/tags/tensorflow"><kbd class="item-tag">tensorflow</kbd></a> 
          </h6>
        </div>
      </div>
    </div>
  </div>
</header>

    <section class="postContent">
  <div class="container">
    <div class="row">
      
      <div class="col-sm-2 col-lg-2">
        <nav id="toc" data-toggle="toc" class="sticky-top"></nav>
      </div>
      
      <div class="col-lg-10 col-sm-10">
        <div class="container blogPost">
          

<h2 id="1-introduction">1. Introduction</h2>

<p>We have introduced the concept of the linear-regression problem and the structure to solve it in a &ldquo;machine-learning&rdquo; fashion in <a href="/blog/linearregression_part1/">this</a> post, while we have applied the theory to a simple but practical case of linear-behavior identification from a bunch of data that are generated in a synthetic way <a href="/blog/linearregression_part2/">here</a> and extend the analysis to a multi-linear case where more than one feature (or input) are fed to the model to predict the outcome <a href="/blog/linearregression_part3/">here</a>.</p>

<p>We now translate the implementation process with popular libraries available in the Python framework, namely <a href="https://scikit-learn.org/stable/">Sklearn</a> and <a href="https://www.tensorflow.org/">Tensorflow</a>.</p>

<p>Scikit-learn is a free software machine learning library for the Python programming language. It enables fast implementation of several classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting and k-means, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.</p>

<p>TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is mainly employed for machine learning applications and more recently deep learning modeling. It is developed by the <a href="https://ai.google/research/teams/brain">Google Brain</a> team.</p>

<p>Code-wise, such libraries let developers focus more on the model itself and achieving an overall better performance by optimizing the model hyper-parameters and by combining different models to deliver an ensemble version out of it.</p>

<p>We are going to implement the logic in Scikit-learn (SKL) first and then in Tensorflow (TF).
The last sections of this post will take care of fundamental aspects of machine learning theory, such as <a href="https://en.wikipedia.org/wiki/Feature_scaling">feature scaling</a>, feature augmentation, via techniques such as <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">polynomial features</a>, and <a href="https://en.wikipedia.org/wiki/Hypothesis">hypothesis evaluation</a>. The last step translates into splitting the data set in at least two sub-sets, namely <strong>training</strong> and <strong>testing</strong>. The former is used to find optimal parameters of the model, the latter is adopted to evaluate the <em>real</em> performance of the model when it is fed with data that have never been seen during training.
One major challenge is to collect large enough training set to find out the optimal parameters, but be able to keep apart a set that can still represent the actual data distribution of the world the model will be employed in. More details of this practise can be found <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">here</a>.</p>

<h2 id="2-data-generation">2. Data generation</h2>

<p>We are going to build three datasets:
1. A multi-linear model of two inputs.
2. A multi-linear model of two inputs, where one input outscales the other one.
3. A multi-linear model of two inputs, where one of them represents polynomial features.</p>

<h3 id="2-1-a-multi-linear-model-of-two-inputs">2.1 A multi-linear model of two inputs</h3>

<p>We start generating some synthetic data ($ N_{pnts} = 50*50 $ points).
We assume we know both the slope of the two inputs ($ \omega_1 = 3, \omega_2 = -1 $) and the intercept ($ \omega_0 = 5 $) of the plane we want to identify, but we also introduce some noise with a gaussian distribution and zero-mean to the plane to make the data source a bit closer to real-world scenarios.
The chart shows the generated data cloud (see <a href="LR-P3">this</a> post for further details).
Here follows the mathematical expression of the model:</p>

<p>$ y = \omega_0 + \omega_1\cdot x_1 + \omega_2\cdot x_2 $</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
from mpl_toolkits import mplot3d

Npntx, Npnty = 50, 50 # number of points
x1_ = np.linspace(-1, 1, Npntx)
x2_ = np.linspace(-1, 1, Npnty)
xx1, xx2 = np.meshgrid(x1_, x2_)
noise = 0.25*(np.random.randn(Npnty,Npntx)-1)
w0, w1, w2 = 5, -3, -1
yy = w0 + w1*xx1 + w2*xx2 + noise
zz = w0 + w1*xx1 + w2*xx2
visData1 = [xx1, xx2, yy, [w0, w1, w2]]
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, zz, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
plt.ylabel(&quot;Y&quot;)

ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')
ax.view_init(30, 35)
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_4_0.png" alt="png" /></p>

<p>The dataset is generated by creating two 2D arrays, one for inputs and one for output.
The input array, XX, is the horizontal concatenation of the flattened version of the two input arrays, <em>xx1</em> and <em>xx2</em>.
There is no need to add the column filled with 1s, as we had to do in the Numpy implementation.</p>

<p>We first stack the two 1D arrays vertically and then transpose it to get the examples ($50*30=1500$) over the rows and the features over the columns (2).
The output 2D array is just a single column filled with the <em>y</em> values. Here the shape of the arrays.</p>

<pre><code class="language-python">XX1 = np.vstack((xx1.flatten(), xx2.flatten())).T
YY1 = yy.flatten().reshape(-1,1)
print([XX1.shape, YY1.shape])
</code></pre>

<pre><code>[(2500, 2), (2500, 1)]
</code></pre>

<h3 id="2-2-a-multi-linear-model-of-two-inputs-where-one-input-outscales-the-other-one">2.2 A multi-linear model of two inputs, where one input outscales the other one</h3>

<p>We generate some synthetic data ($ N_{pnts} = 50*50 $ points), but we make sure one input maximum value is far greater than the other one. In particular, <em>x1</em> scales from -1000 to 1000, while <em>x2</em> from -1 to 1.
However, the mathematical correlation does not change:</p>

<p>$ y = \omega_0 + \omega_1\cdot x_1 + \omega_2\cdot x_2 $</p>

<p>In real-life task, it is common to face such situations. Credit risk management is one example, where some inputs to the model could be how many employees work for the company that should take the loan, and the annual revenue.
The order of magnitude of the latter is way too greater than the former, in general.</p>

<p>You can see in the below chart how the second input looks like to have no impact ot the outcome of the model.</p>

<pre><code class="language-python">Npntx, Npnty = 50, 50 # number of points
x1_ = np.linspace(-100, 100, Npntx)
x2_ = np.linspace(-1, 1, Npnty)
xx1, xx2 = np.meshgrid(x1_, x2_)
noise = 0.25*(np.random.randn(Npnty,Npntx)-1)
w0, w1, w2 = 2, -3, -1
yy = w0 + w1*xx1 + w2*xx2 + noise
zz = w0 + w1*xx1 + w2*xx2
visData2 = [xx1, xx2, yy, [w0, w1, w2]]
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, zz, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)

ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
plt.ylabel(&quot;Y&quot;)

ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')
ax.view_init(30, 35)
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_9_0.png" alt="png" /></p>

<p>The dataset is generated with the same procedure.</p>

<pre><code class="language-python">XX2 = np.vstack((xx1.flatten(), xx2.flatten())).T
YY2 = yy.flatten().reshape(-1,1)
print([XX2.shape, YY2.shape])
</code></pre>

<pre><code>[(2500, 2), (2500, 1)]
</code></pre>

<h3 id="2-3-a-multi-linear-model-of-two-inputs-where-one-of-them-represents-polynomial-features">2.3 A multi-linear model of two inputs, where one of them represents polynomial features.</h3>

<p>We generate some synthetic data ($ N_{pnts} = 50*50 $ points), where the first feature <em>x1</em> shows off as a quadratic function.
The mathematical correlation is as follows:</p>

<p>$ y = \omega_0 + \omega_1\cdot x_1 + \omega_2\cdot x_1^2 + \omega_3\cdot x_2 $</p>

<p>In real-life task, it is common to face such situations. Joule heating is one example, where the heat released by a light bulb is correlated to the square of the electric current through the wires.</p>

<p>You can see in the below chart how the first input is responsible of the curvature of the generated surface.</p>

<pre><code class="language-python">Npntx, Npnty = 50, 50 # number of points
x1_ = np.linspace(-5, 5, Npntx)
x2_ = np.linspace(-5, 5, Npnty)
xx1, xx2 = np.meshgrid(x1_, x2_)
noise = 0.25*(np.random.randn(Npnty,Npntx)-1)
w0, w1, w2, w3 = 2, -3, -1, 2
yy = w0 + w1*xx1 + w2*xx1**2 + w3*xx2 + noise
zz = w0 + w1*xx1 + w2*xx1**2 + w3*xx2
visData3 = [xx1, xx2, yy, [w0, w1, w2, w3]]
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, zz, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)

ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
plt.xlabel(&quot;X1&quot;)
plt.ylabel(&quot;X2&quot;)
plt.ylabel(&quot;Y&quot;)

ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y')
ax.view_init(30, 35)
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_14_0.png" alt="png" /></p>

<p>The dataset is generated with the same procedure.</p>

<pre><code class="language-python">XX3 = np.vstack((xx1.flatten(), xx1.flatten()**2, xx2.flatten())).T
YY3 = yy.flatten().reshape(-1,1)
print([XX3.shape, YY3.shape])
</code></pre>

<pre><code>[(2500, 3), (2500, 1)]
</code></pre>

<h2 id="3-linear-regression-with-scikit-learn">3. Linear regression with Scikit-learn</h2>

<p>We import the module required to define the linear model, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"><strong>LinearRegression</strong></a>, from the <em>linear_model</em> package, and the module to evaluate the performance of the model, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"><strong>RMSE</strong></a>, from the <em>metrics</em> package.
It is enough to fit the model parameters to the first dataset and to calculate the model prediction for the inputs of each sample of the same dataset.</p>

<p>We can realize (and appreciate) how the overall code ends up being much more compact and easier to write and maintain.</p>

<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lm = LinearRegression()
lm.fit(XX1, YY1)
Ypred = lm.predict(XX1)
print('The final RSME is : {}'.format(mean_squared_error(YY1, Ypred)))
print('The final parameter values: {}'.format(np.hstack((lm.intercept_, lm.coef_[0,:])).tolist()))
</code></pre>

<pre><code>The final RSME is : 0.060867550456315374
The final parameter values: [4.753763678286057, -3.001900368199036, -1.0124158793115567]
</code></pre>

<pre><code class="language-python">xx1, xx2, yy = visData1
ypred = Ypred.reshape(-1, xx1.shape[-1])
plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, ypred, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 30)

plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_21_0.png" alt="png" /></p>

<p>In three lines of Python code we are now able to perform what requires a lot of effort and coding when starting from scratch.</p>

<h2 id="4-linear-regression-with-tensorflow">4. Linear regression with TensorFlow</h2>

<p>We import the entire library, from which we access to the various methods required to describe the model, to train it to the dataset and to estimate the outputs that are compared to the dataset ground-truth values.</p>

<h3 id="4-1-model-definition">4.1 Model definition</h3>

<p>The very first step is to reset the TF to the default graph, which means TF clears the default graph stack and resets the global default graph.</p>

<p>We then define the <em>x</em> and <em>y</em> variables as <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder"><strong>placeholder</strong></a>, while the <em>ww</em> parameters as <a href="https://www.tensorflow.org/guide/variables"><strong>variable</strong></a>.</p>

<p>In short, <em>tf.Variable</em> is used for trainable parameters of the model, while <em>tf.placeholder</em> is used to feed actual training examples.
That&rsquo;s why we need to assign initial values, often random-generated, to the TF variables only.
The variable values can therefore be updated during optimization, can be shared and be stored after training.
We assign the placeholder type as <em>float32</em> to both input and output.
The size of the input placeholder, <em>xp</em>, is set to (None, 2), since the number of rows is automatically determined from the batch size we feed to the optimizer object in the training step, while the column size is equal to the number of features (2 for the first case).
The size of the output placeholder is instead set to (None, 1), since only one value is required for each sample.</p>

<p>The feature weights <em>ww</em> and bias <em>bb</em>, which is equivalent to the Scikit-Learn intercept, are defined with the <em>Variable</em> method and initialized as a (2,1) and a (1,1) zero-arrays, respectively.</p>

<p>The final step is to combine TF variables and placeholders to translate the mathematical model into code.
The matrix multiplication between the input matrix and the weight array is performed with <em>matmul</em>.
At the end of these steps, we inspect the shape of each tensor.
The question-mark symbol says that TF needs some data to determine the actual row size.</p>

<pre><code class="language-python">import tensorflow as tf

tf.reset_default_graph()
xp = tf.placeholder(dtype=tf.float32, shape=(None, 2))
yp = tf.placeholder(dtype=tf.float32, shape=(None, 1))
ww = tf.Variable(np.zeros((2,1)), dtype=tf.float32)
bb = tf.Variable(np.zeros((1,1)), dtype=tf.float32)
ymdl = tf.matmul(xp, ww) + bb
</code></pre>

<pre><code class="language-python">print('Input shape: {}'.format(xp.shape))
print('Ground-truth output shape: {}'.format(yp.shape))
print('Weight shape: {}'.format(ww.shape))
print('Model output shape: {}'.format(ymdl.shape))
</code></pre>

<pre><code>Input shape: (?, 2)
Ground-truth output shape: (?, 1)
Weight shape: (2, 1)
Model output shape: (?, 1)
</code></pre>

<p>The loss function is easily implemented using the method <em>mean_squared_error</em> from <em>losses</em> package.
The optimizer object that actually adjusts the model parameters (TF variables) with the gradient descent algorithm.</p>

<pre><code class="language-python">mdlLoss = tf.losses.mean_squared_error(yp, ymdl)
optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss=mdlLoss)
</code></pre>

<h3 id="4-2-model-training">4.2 Model training</h3>

<p>The next steps are to:
1. initialize the variables.
2. run a new session, which let us perform the actual computation by exploitng the graph structure previousluy defined.
3. run the optimizer as many steps as the numner of epochs <em>Nepoch</em>.
4. run the model with the final parameter set and store the mdoel output <em>ymdl</em> into the prediction array.
5. retrieve the final parameter values by running a dedicated session. A different way would be to call the <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables">global_variables()</a> method and get the variable values by key name.</p>

<pre><code class="language-python">Nepoch = 5000
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    Jevol = []
    for kk in range(Nepoch):
        mdl_loss, _ = sess.run([mdlLoss, optimizer], feed_dict={xp: XX1, yp: YY1})
        if kk%100 == 0:
            Jevol.append((kk, mdl_loss))
        if kk==Nepoch-1:
            print('The final model loss is {}'.format(mdl_loss))

    Ypred_tf = sess.run(ymdl, feed_dict={xp: XX1})
    bOpt, wOpt = sess.run([bb, ww])
</code></pre>

<pre><code>The final model loss is 0.0608675591647625
</code></pre>

<pre><code class="language-python">Jevol = np.array(Jevol)
plt.figure(figsize=(10, 5))
plt.plot(Jevol[:,0], np.log(Jevol[:,1]), lw=2)
plt.xlabel(&quot;training steps ($N_{epoch}$)&quot;)
plt.ylabel(&quot;Logarithm loss trend ($log(J_{evol})$)&quot;)
plt.title('The model loss over the training epochs')
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_32_0.png" alt="png" /></p>

<pre><code class="language-python">print('The final RSME is : {}'.format(mean_squared_error(YY1, Ypred_tf)))
print('The final parameter values: {}'.format(np.vstack((bOpt, wOpt))[:,0].tolist()))
</code></pre>

<pre><code>The final RSME is : 0.060867550351984094
The final parameter values: [4.753759384155273, -3.001899242401123, -1.012415885925293]
</code></pre>

<pre><code class="language-python">xx1, xx2, yy = visData1
ypredTF = Ypred_tf.reshape(-1, xx1.shape[-1])
plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, ypredTF, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 30)
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_34_0.png" alt="png" /></p>

<h2 id="5-feature-scaling">5. Feature scaling</h2>

<p>Feature scaling can impact the final performance of some algorithms to a great extent, while might have a minimal or no effect in others.
From a theoretical point of view, feature scaling should impact mostly algorithms based on the Euclidean distance.
If one of the features varies over a much broader range, the distance will be ruled by this feature.
Another reason to apply feature scaling is that it helps the gradient descent algorithm to converge much faster.</p>

<p>We want to investigate the effects of feature scaling for dataset 2 and 3 and show how linear regression has no benefits into applying feature scaling in terms of final outcome.</p>

<h3 id="5-1-dataset-2">5.1 Dataset 2</h3>

<p>We first run the same SKL model on dataset 2, without any preprocess of the features.
Recall that the maximum value of the first input is far greater than the second one.
In particular, <em>x1</em> scales from -1000 to 1000, while <em>x2</em> from -1 to 1.</p>

<pre><code class="language-python">lm2 = LinearRegression() # linear model without feature-scaling

lm2.fit(XX2, YY2)
Ypred2 = lm2.predict(XX2)
</code></pre>

<pre><code class="language-python">xx1, xx2, yy, wws = visData2
print('The final RSME is : {}'.format(mean_squared_error(YY2, Ypred2)))
print('The original parameter values: {}'.format(wws))
print('The final parameter values: {}'.format(np.hstack((lm2.intercept_, lm2.coef_[0,:])).tolist()))
</code></pre>

<pre><code>The final RSME is : 0.06319048495495569
The original parameter values: [2, -3, -1]
The final parameter values: [1.7556085903611112, -2.9999176976634137, -0.9975157271349785]
</code></pre>

<pre><code class="language-python">ypred2 = Ypred2.reshape(-1, xx1.shape[-1])
plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, ypred2, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 30)
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_40_0.png" alt="png" /></p>

<p>The gradient descent algorithm is able to find the optimal parameter values that minimize the loss function to the data-noise level.</p>

<h3 id="5-2-dataset-3">5.2 Dataset 3</h3>

<p>We run the same SKL model on dataset 3, without any preprocess of the features.
Recall that the first feature <em>x1</em> shows off as a quadratic function.</p>

<pre><code class="language-python">lm3 = LinearRegression() # linear model without feature-scaling

lm3.fit(XX3, YY3)
Ypred3 = lm3.predict(XX3)
</code></pre>

<pre><code class="language-python">xx1, xx2, yy, wws = visData3
print('The final RSME is : {}'.format(mean_squared_error(YY3, Ypred3)))
print('The original parameter values: {}'.format(wws))
print('The final parameter values: {}'.format(np.hstack((lm3.intercept_, lm3.coef_[0,:])).tolist()))
</code></pre>

<pre><code>The final RSME is : 0.06302815103212135
The original parameter values: [2, -3, -1, 2]
The final parameter values: [1.7457329949127356, -2.9991028003507973, -1.0006755077021603, 2.001083379548741]
</code></pre>

<pre><code class="language-python">ypred3 = Ypred3.reshape(-1, xx1.shape[-1])
plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, ypred3, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 30)
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_46_0.png" alt="png" /></p>

<p>The gradient descent algorithm is able to find the optimal parameter values that minimize the loss function to the data-noise level.</p>

<h2 id="6-polynomial-regression">6. Polynomial regression</h2>

<h3 id="6-1-second-degree-polynomial">6.1 Second-degree polynomial</h3>

<p>We import the method <em>PolynomialFeatures</em> from SKL package <em>preprocessing</em>.
We extract the first-order terms of the two features from the original input matrix <em>XX3</em> by removing the square term of the first feature, which is contained in the second column.
We define the degree of the polynomial of the two features and transform the input matrix. Since we apply second-degree polynomial transformation to two features, we obtain 6 columns at the end, as follows:</p>

<p>$ (x_1, x_2) \rightarrow (1, x_1, x_2, x_1^2, x_1\cdot x_2, x_2^2)$ (1)</p>

<p>We then feed the new matrix into the linear model object to get the optimal weights.</p>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

Xpr = np.delete(XX3, 1, axis=1) # drop (1-indexed) second column along axis=1
PF = PolynomialFeatures(degree=2)
Xpr = PF.fit_transform(Xpr)
print('The input matrix comes with {} features/columns'.format(Xpr.shape[-1]))
</code></pre>

<pre><code>The input matrix comes with 6 features/columns
</code></pre>

<pre><code class="language-python">lm4 = LinearRegression()
lm4.fit(Xpr, YY3)
Ypred4 = lm4.predict(Xpr)
</code></pre>

<pre><code class="language-python">xx1, xx2, yy, wws = visData3
print('The final RSME is : {}'.format(mean_squared_error(YY3, Ypred4)))
print('The original parameter values: {}'.format(wws))
print('The final parameter values: {}'.format(np.hstack((lm4.intercept_, lm4.coef_[0,1:])).tolist()))
</code></pre>

<pre><code>The final RSME is : 0.06297591725187177
The original parameter values: [2, -3, -1, 2]
The final parameter values: [1.750110081427339, -2.9991028003507987, 2.00108337954874, -1.00067550770216, -0.0007005962684500263, -0.0005046523275718696]
</code></pre>

<p>The order of the final parameter set is given by the expression (1). Two terms, $x_1\cdot x_2$ and $x_2^2$, received almost 0-weight, which means the algorithm is able to identify which polynomial feature is required to describe the phenomenon.
However, it is safe to introduce some techniques to prevent the algorithm to learn overfit.
Indeed, it could happen that too complex functions that react in an unreasonable way to extreme input values, even though their associated weights are small.
Overfitting is a critical topic for machine learning.
A dedicated post will be realised soon.</p>

<p>We use the new model to generate the surface corresponding to a broader input domain $(-10, 10) \times (-10, 10)$, while the data could has been generated for the domain $(-5, 5) \times (-5, 5)$.</p>

<pre><code class="language-python">Npntx, Npnty = 50, 50 # number of points
x1_ = np.linspace(-10, 10, Npntx)
x2_ = np.linspace(-10, 10, Npnty)
xx1L, xx2L = np.meshgrid(x1_, x2_)

Xlarge = np.vstack((xx1L.flatten(), xx2L.flatten())).T
XLP = PF.fit_transform(Xlarge)
Ypred4L = lm4.predict(XLP)
ypred4L = Ypred4L.reshape(-1, xx1L.shape[-1])
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1L, xx2L, ypred4L, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 20)
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_56_0.png" alt="png" /></p>

<h3 id="6-2-third-degree-polynomial">6.2 Third-degree polynomial</h3>

<p>We now apply third-degree polynomial transformation to two features and obtain 10 columns at the end, as follows:</p>

<p>$ (x_1, x_2) \rightarrow (1, x_1, x_2, x_1^2, x_1\cdot x_2, x_2^2, x_1^3, x_1^2\cdot x_2, x_1\cdot x_2^2, x_2^3)$ (2)</p>

<p>We then feed the new matrix into the linear model object to get the optimal weights.</p>

<pre><code class="language-python">Xpr = np.delete(XX3, 1, axis=1) # drop (1-indexed) second column along axis=1
PF = PolynomialFeatures(degree=3)
Xpr = PF.fit_transform(Xpr)
print('The input matrix comes with {} features/columns'.format(Xpr.shape[-1]))
</code></pre>

<pre><code>The input matrix comes with 10 features/columns
</code></pre>

<p>You can easily understand how SKL handles the polynomial generation with this simple array transformation.</p>

<pre><code class="language-python">print(PF.fit_transform(np.array([[2,3]])))
</code></pre>

<pre><code>[[  1.   2.   3.   4.   6.   9.   8.  12.  18.  27.]]
</code></pre>

<pre><code class="language-python">lm5 = LinearRegression()
lm5.fit(Xpr, YY3)
Ypred5 = lm5.predict(Xpr)
</code></pre>

<pre><code class="language-python">xx1, xx2, yy, wws = visData3
print('The final RSME is : {}'.format(mean_squared_error(YY3, Ypred5)))
print('The original parameter values: {}'.format(wws))
print('The final parameter values: {}'.format(np.hstack((lm5.intercept_, lm5.coef_[0,1:])).tolist()))
</code></pre>

<pre><code>The final RSME is : 0.06292996557343045
The original parameter values: [2, -3, -1, 2]
The final parameter values: [1.7501100814273354, -2.9986611741153584, 2.00081744310528, -1.00067550770216, -0.0007005962684500823, -0.0005046523275716124, 5.3787097770249686e-05, -0.00022137986615712604, -0.00014768202685457046, 0.00014009740059971174]
</code></pre>

<p>The order of the final parameter set is given by the expression (2). Whatever feature was not used to generate the dataset has received a closed to 0 weight. However, since some features are powered to 3, the overfitting problem and then the risk of unstable behaviour of the model are even more severe and critical.</p>

<p>We use the new model to generate the surface corresponding to a broader input domain $(-10, 10) \times (-10, 10)$, while the data could has been generated for the domain $(-5, 5) \times (-5, 5)$.</p>

<pre><code class="language-python">Npntx, Npnty = 50, 50 # number of points
x1_ = np.linspace(-10, 10, Npntx)
x2_ = np.linspace(-10, 10, Npnty)
xx1L, xx2L = np.meshgrid(x1_, x2_)

Xlarge = np.vstack((xx1L.flatten(), xx2L.flatten())).T
XLP = PF.fit_transform(Xlarge)
Ypred5L = lm5.predict(XLP)
ypred5L = Ypred5L.reshape(-1, xx1L.shape[-1])
</code></pre>

<pre><code class="language-python">plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1L, xx2L, ypred5L, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 20)
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_65_0.png" alt="png" /></p>

<h2 id="7-hypothesis-evaluation">7. Hypothesis evaluation</h2>

<p>The last chapter of this post concerns the capability of the learned model to generalise to new samples.
It means we want our model to be reliable in real-wolrd applications when it is going to face new inputs that have not been encounter before, i.e., during training.</p>

<p>We split the complete dataset into two sets by using the 80-20 rule, which implies we are going to use 80% of the data to train the model parameters and 20% of it to test the actual performance.
The dataset needs to be shuffled before being split to prevent the two new datasets from bearing different statistical content.
Luckly for us, it is implemented in the <em>train_test_split</em> method from <em>model_selection</em> package available in SKL.</p>

<pre><code class="language-python">from sklearn.model_selection import train_test_split

Xtrain, Xtest, Ytrain, Ytest = train_test_split(XX3, YY3, test_size=0.2, random_state=42)

lm6 = LinearRegression()
lm6.fit(Xtrain, Ytrain)
Ypred6 = lm6.predict(Xtest)
</code></pre>

<pre><code class="language-python">xx1, xx2, yy, wws = visData3
print('The final RSME over training set is : {}'.format(mean_squared_error(Ytrain, lm6.predict(Xtrain))))
print('The final RSME over complete set is : {}'.format(mean_squared_error(YY3, lm6.predict(XX3))))
print('The final RSME over testing set is : {}'.format(mean_squared_error(Ytest, lm6.predict(Xtest))))
print('-'*80)
print('The original parameter values: {}'.format(wws))
print('The final parameter values: {}'.format(np.hstack((lm6.intercept_, lm6.coef_[0,:])).tolist()))
</code></pre>

<pre><code>The final RSME over training set is : 0.06205556313356443
The final RSME over complete set is : 0.06303448176658248
The final RSME over testing set is : 0.0669501562986547
--------------------------------------------------------------------------------
The original parameter values: [2, -3, -1, 2]
The final parameter values: [1.7429651595022575, -2.9995190644738403, -1.0004142598405084, 2.0008497763561404]
</code></pre>

<p>The algorithm is still able to identify the proper model that describes the data.
However, model error is slightly higher over testing set.</p>

<pre><code class="language-python">ypred6 = lm6.predict(XX3).reshape(-1, xx1.shape[-1])
plt.figure(figsize=(10, 5))
ax = plt.axes(projection='3d')
ax.plot_surface(xx1, xx2, ypred6, rstride=1, cstride=1, cmap='viridis', edgecolor='none', alpha=0.5)
ax.scatter(xx1, xx2, yy, cmap='viridis', linewidth=0.5, alpha=0.5)
ax.set_xlabel('$x_1$')
ax.set_ylabel('$x_2$')
ax.set_zlabel('y')
ax.view_init(20, 20)
plt.tight_layout()
plt.show()
</code></pre>

<p><img src="/blog/linReg4/output_72_0.png" alt="png" /></p>

<h2 id="reference">Reference</h2>

<ol>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">CS229 notes</a></li>
<li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning at Coursera</a></li>
<li><a href="http://people.math.gatech.edu/~ecroot/3225/maximum_likelihood.pdf">Maximum likelihood estimators and least squares</a></li>
<li><a href="https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf">An Introduction to Statistical Learning</a></li>
</ol>

        </div>
        <div class="pgNav PageNavigation col-12 text-center">
          <span>
          <p>&laquo; <a class="" href="/blog/linearregression_part3/" style="color: #4ABDAC; font-size: 18px; "> Hello world example for Machine learning - Part 3</a>

          
          &nbsp;&nbsp; | &nbsp;&nbsp;
          <a class="" href="/blog/logisticregression_part1/" style="color: #4ABDAC; font-size: 18px; ">How to learn to classify - Part 1</a>
          
          &raquo;</p>
          </span>
          
        </div>

      </div>
    </div>
  </div>
</section>



    
    

    

    <footer class="pgFoot">
  <div class="container-fluid">
    <div class="row">
      <div class="col-12 text-center icons">
        
        <span>
          <a href="https://github.com/takeawildguess/"><i class="fa fa-github"></i></a><a href="https://twitter.com/takeawildguess4/"><i class="fa fa-twitter"></i></a><a href="https://www.linkedin.com/in/mattia-venditti-9137a124/"><i class="fa fa-linkedin"></i></a>
        </span>
        
      </div>

      <hr style="border: 2px solid #4ABDAC; min-width: 250px; border-radius: 2px; " />
      <div class="col-12 text-center">
        <p>
          &bull; Copyright &copy; 2019, <a href="https://takeawildguess.net/">Mattia Venditti</a> &bull; All rights reserved. &bull;
          <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" />
          </a>
          All blog posts are released under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
            Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
        </p>
      </div>
      

      <div class="col-6 text-center">
        <p>Disclaimer: The views and opinions on this website are my own and do not reflect or represent the views of my employer.</p>
      </div>
      <div class="col-6 text-center">
        <p>
        Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://pages.github.com/">GitHub Pages</a>.
        The favicon and logo were created by myself.
        </p>
      </div>

    </div>
  </div>
</footer>

<a id="back-to-top" href="https://takeawildguess.net/" class="btn btn-primary btn-lg back-to-top" role="button" title="Click to return on the top page"
data-toggle="tooltip" data-placement="left"><i class="fa fa-angle-up"></i></a>


  </body>
</html>
